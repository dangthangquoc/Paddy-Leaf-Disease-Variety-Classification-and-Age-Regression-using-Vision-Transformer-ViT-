{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ad58b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Sequential, Model, Input\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "random.seed(45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4ac3b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **IMPROVEMENT 1: Add comprehensive logging and visualization**\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1697d101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **IMPROVEMENT 2: Create class to manage experiments**\n",
    "class ExperimentManager:\n",
    "    def __init__(self, experiment_name):\n",
    "        self.experiment_name = experiment_name\n",
    "        self.results_dir = f\"results/{experiment_name}\"\n",
    "        os.makedirs(self.results_dir, exist_ok=True)\n",
    "        self.results = {}\n",
    "    \n",
    "    def log_experiment(self, model_name, metrics, config):\n",
    "        self.results[model_name] = {\n",
    "            'metrics': metrics,\n",
    "            'config': config\n",
    "        }\n",
    "        \n",
    "        # Save results to JSON\n",
    "        import json\n",
    "        with open(f\"{self.results_dir}/results.json\", 'w') as f:\n",
    "            json.dump(self.results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85f0ee7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **IMPROVEMENT 3: Advanced data analysis and visualization**\n",
    "def analyze_dataset_comprehensive(meta_train_df):\n",
    "    \"\"\"Perform comprehensive dataset analysis for HD/DI requirements\"\"\"\n",
    "    \n",
    "    # Class imbalance analysis\n",
    "    class_distribution = meta_train_df['label'].value_counts()\n",
    "    imbalance_ratio = class_distribution.max() / class_distribution.min()\n",
    "    \n",
    "    # Create visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(20, 16))\n",
    "    \n",
    "    # Plot 1: Class distribution\n",
    "    class_distribution.plot(kind='bar', ax=axes[0,0])\n",
    "    axes[0,0].set_title(f'Class Distribution (Imbalance Ratio: {imbalance_ratio:.2f}:1)')\n",
    "    axes[0,0].set_xlabel('Disease Class')\n",
    "    axes[0,0].set_ylabel('Number of Images')\n",
    "    \n",
    "    # Plot 2: Variety distribution\n",
    "    variety_distribution = meta_train_df['variety'].value_counts()\n",
    "    variety_distribution.plot(kind='bar', ax=axes[0,1])\n",
    "    axes[0,1].set_title('Variety Distribution')\n",
    "    axes[0,1].set_xlabel('Paddy Variety')\n",
    "    axes[0,1].set_ylabel('Number of Images')\n",
    "    \n",
    "    # Plot 3: Age distribution\n",
    "    axes[1,0].hist(meta_train_df['age'], bins=30, edgecolor='black')\n",
    "    axes[1,0].set_title('Age Distribution')\n",
    "    axes[1,0].set_xlabel('Age (days)')\n",
    "    axes[1,0].set_ylabel('Frequency')\n",
    "    \n",
    "    # Plot 4: Class-Variety heatmap\n",
    "    cross_table = pd.crosstab(meta_train_df['label'], meta_train_df['variety'])\n",
    "    sns.heatmap(cross_table, cmap='YlOrRd', annot=True, fmt='d', ax=axes[1,1])\n",
    "    axes[1,1].set_title('Class-Variety Distribution Heatmap')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('dataset_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    return {\n",
    "        'class_distribution': class_distribution,\n",
    "        'imbalance_ratio': imbalance_ratio,\n",
    "        'variety_distribution': variety_distribution,\n",
    "        'age_stats': {\n",
    "            'mean': meta_train_df['age'].mean(),\n",
    "            'std': meta_train_df['age'].std(),\n",
    "            'min': meta_train_df['age'].min(),\n",
    "            'max': meta_train_df['age'].max()\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6074220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **IMPROVEMENT 4: Advanced data augmentation with class balancing**\n",
    "def create_balanced_data_generator(train_df, batch_size=32, img_size=(256, 256)):\n",
    "    \"\"\"Create data generator with class balancing and variety-aware sampling\"\"\"\n",
    "    \n",
    "    # Calculate class weights\n",
    "    class_weights = compute_class_weight(\n",
    "        'balanced',\n",
    "        classes=np.unique(train_df['label']),\n",
    "        y=train_df['label']\n",
    "    )\n",
    "    class_weight_dict = dict(enumerate(class_weights))\n",
    "    \n",
    "    # Advanced augmentation pipeline\n",
    "    class_specific_augmentation = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "        rescale=1.0/255.0,\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.15,\n",
    "        height_shift_range=0.15,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True,\n",
    "        zoom_range=0.15,\n",
    "        fill_mode='nearest',\n",
    "        # More aggressive augmentation for minority classes\n",
    "        shear_range=0.15,\n",
    "        brightness_range=[0.8, 1.2],\n",
    "        channel_shift_range=20.0\n",
    "    )\n",
    "    \n",
    "    return class_specific_augmentation, class_weight_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cbdf36b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **IMPROVEMENT 5: Enhanced ViT with attention visualization**\n",
    "class EnhancedViT(tf.keras.Model):\n",
    "    def __init__(self, num_classes, **kwargs):\n",
    "        super(EnhancedViT, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.attention_weights = []\n",
    "        \n",
    "        # Build the model architecture\n",
    "        self.data_augmentation = keras.Sequential([\n",
    "            layers.Normalization(),\n",
    "            layers.Resizing(72, 72),\n",
    "            layers.RandomFlip(\"horizontal\"),\n",
    "            layers.RandomRotation(factor=0.05),\n",
    "            layers.RandomZoom(height_factor=0.3, width_factor=0.3),\n",
    "            layers.RandomContrast(factor=0.2),  # Added\n",
    "            layers.RandomBrightness(factor=0.2)  # Added\n",
    "        ], name=\"data_augmentation\")\n",
    "        \n",
    "        self.patches = Patches(patch_size=6)\n",
    "        self.patch_encoder = PatchEncoder(num_patches=144, projection_dim=64)\n",
    "        \n",
    "        # Transformer layers with attention weight collection\n",
    "        self.transformer_blocks = []\n",
    "        for i in range(8):\n",
    "            transformer_block = TransformerBlock(\n",
    "                embed_dim=64,\n",
    "                num_heads=4,\n",
    "                ff_dim=128,\n",
    "                rate=0.1,\n",
    "                name=f\"transformer_block_{i}\"\n",
    "            )\n",
    "            self.transformer_blocks.append(transformer_block)\n",
    "        \n",
    "        self.layernorm = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.dropout = layers.Dropout(0.5)\n",
    "        \n",
    "        # Enhanced MLP head\n",
    "        self.mlp_head = keras.Sequential([\n",
    "            layers.Dense(2048, activation='gelu'),\n",
    "            layers.Dropout(0.3),\n",
    "            layers.Dense(1024, activation='gelu'),\n",
    "            layers.Dropout(0.3),\n",
    "            layers.Dense(512, activation='gelu'),\n",
    "            layers.Dropout(0.2),\n",
    "            layers.Dense(num_classes)\n",
    "        ])\n",
    "    \n",
    "    def call(self, inputs, training=None, return_attention=False):\n",
    "        # Data augmentation\n",
    "        x = self.data_augmentation(inputs)\n",
    "        \n",
    "        # Create patches and encode\n",
    "        patches = self.patches(x)\n",
    "        encoded_patches = self.patch_encoder(patches)\n",
    "        \n",
    "        # Pass through transformer blocks\n",
    "        self.attention_weights = []\n",
    "        for transformer in self.transformer_blocks:\n",
    "            encoded_patches, attention_weights = transformer(encoded_patches, return_attention=True)\n",
    "            if return_attention:\n",
    "                self.attention_weights.append(attention_weights)\n",
    "        \n",
    "        # Final processing\n",
    "        representation = self.layernorm(encoded_patches)\n",
    "        representation = self.flatten(representation)\n",
    "        representation = self.dropout(representation, training=training)\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.mlp_head(representation)\n",
    "        \n",
    "        if return_attention:\n",
    "            return logits, self.attention_weights\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c399c67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **IMPROVEMENT 6: Cross-validation with stratified sampling**\n",
    "def stratified_cross_validation(X, y, n_splits=5):\n",
    "    \"\"\"Perform stratified k-fold cross-validation\"\"\"\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    cv_results = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "        logger.info(f\"Training fold {fold + 1}/{n_splits}\")\n",
    "        \n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        # Create and train model for this fold\n",
    "        model = create_enhanced_vit_classifier()\n",
    "        \n",
    "        # Train with early stopping and learning rate scheduling\n",
    "        callbacks = [\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                monitor='val_accuracy',\n",
    "                patience=15,\n",
    "                restore_best_weights=True\n",
    "            ),\n",
    "            tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                monitor='val_accuracy',\n",
    "                factor=0.5,\n",
    "                patience=7,\n",
    "                min_lr=1e-7\n",
    "            ),\n",
    "            tf.keras.callbacks.ModelCheckpoint(\n",
    "                f'models/fold_{fold}_best_model.keras',\n",
    "                monitor='val_accuracy',\n",
    "                save_best_only=True\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=100,\n",
    "            batch_size=32,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Evaluate fold\n",
    "        y_pred = model.predict(X_val)\n",
    "        y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "        \n",
    "        # Store results\n",
    "        fold_results = {\n",
    "            'fold': fold + 1,\n",
    "            'best_val_accuracy': max(history.history['val_accuracy']),\n",
    "            'final_val_accuracy': history.history['val_accuracy'][-1],\n",
    "            'confusion_matrix': confusion_matrix(y_val, y_pred_classes),\n",
    "            'classification_report': classification_report(y_val, y_pred_classes, output_dict=True)\n",
    "        }\n",
    "        \n",
    "        cv_results.append(fold_results)\n",
    "    \n",
    "    return cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7decd15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **IMPROVEMENT 7: Model ensemble implementation**\n",
    "class ModelEnsemble:\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Ensemble prediction using voting\"\"\"\n",
    "        predictions = []\n",
    "        \n",
    "        for model in self.models:\n",
    "            pred = model.predict(X)\n",
    "            predictions.append(pred)\n",
    "        \n",
    "        # Weighted average (can be adjusted based on individual model performance)\n",
    "        ensemble_pred = np.mean(predictions, axis=0)\n",
    "        return ensemble_pred\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Get probability predictions from ensemble\"\"\"\n",
    "        predictions = []\n",
    "        \n",
    "        for model in self.models:\n",
    "            pred = model.predict(X)\n",
    "            predictions.append(tf.nn.softmax(pred).numpy())\n",
    "        \n",
    "        # Average probabilities\n",
    "        ensemble_proba = np.mean(predictions, axis=0)\n",
    "        return ensemble_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e7de181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **IMPROVEMENT 8: Comprehensive evaluation and visualization**\n",
    "def comprehensive_model_evaluation(model, X_test, y_test, class_names):\n",
    "    \"\"\"Comprehensive evaluation with advanced metrics and visualizations\"\"\"\n",
    "    \n",
    "    # Get predictions\n",
    "    y_pred_probs = model.predict(X_test)\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "    \n",
    "    # Create evaluation directory\n",
    "    eval_dir = \"model_evaluation\"\n",
    "    os.makedirs(eval_dir, exist_ok=True)\n",
    "    \n",
    "    # 1. Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{eval_dir}/confusion_matrix.png', dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Per-class metrics\n",
    "    report = classification_report(y_test, y_pred, target_names=class_names, output_dict=True)\n",
    "    \n",
    "    # Visualize per-class metrics\n",
    "    metrics_df = pd.DataFrame(report).transpose()\n",
    "    metrics_df = metrics_df.drop(['accuracy', 'macro avg', 'weighted avg'])\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    metrics_df[['precision', 'recall', 'f1-score']].plot(kind='bar', ax=ax)\n",
    "    plt.title('Per-Class Performance Metrics')\n",
    "    plt.xlabel('Disease Class')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{eval_dir}/per_class_metrics.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. ROC curves for each class\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    from sklearn.preprocessing import label_binarize\n",
    "    \n",
    "    y_test_bin = label_binarize(y_test, classes=range(len(class_names)))\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for i in range(len(class_names)):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_pred_probs[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "        plt.plot(fpr[i], tpr[i], label=f'{class_names[i]} (AUC = {roc_auc[i]:.3f})')\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curves for Each Class')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{eval_dir}/roc_curves.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 4. Error analysis\n",
    "    error_indices = np.where(y_pred != y_test)[0]\n",
    "    error_analysis = {}\n",
    "    \n",
    "    for idx in error_indices:\n",
    "        true_class = class_names[y_test[idx]]\n",
    "        pred_class = class_names[y_pred[idx]]\n",
    "        confidence = y_pred_probs[idx, y_pred[idx]]\n",
    "        \n",
    "        key = f\"{true_class} -> {pred_class}\"\n",
    "        if key not in error_analysis:\n",
    "            error_analysis[key] = []\n",
    "        \n",
    "        error_analysis[key].append({\n",
    "            'index': idx,\n",
    "            'confidence': confidence\n",
    "        })\n",
    "    \n",
    "    # Visualize error patterns\n",
    "    error_counts = {k: len(v) for k, v in error_analysis.items()}\n",
    "    error_df = pd.DataFrame(list(error_counts.items()), columns=['Error Type', 'Count'])\n",
    "    error_df = error_df.sort_values('Count', ascending=False).head(10)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(data=error_df, x='Count', y='Error Type')\n",
    "    plt.title('Top 10 Most Common Misclassifications')\n",
    "    plt.xlabel('Number of Errors')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{eval_dir}/error_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    return {\n",
    "        'confusion_matrix': cm,\n",
    "        'classification_report': report,\n",
    "        'roc_auc': roc_auc,\n",
    "        'error_analysis': error_analysis\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b3be4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **IMPROVEMENT 9: Attention visualization for interpretability**\n",
    "def visualize_attention_maps(model, image, class_names, save_path='attention_maps.png'):\n",
    "    \"\"\"Visualize attention maps for interpretability\"\"\"\n",
    "    \n",
    "    # Get predictions with attention weights\n",
    "    predictions, attention_weights = model(tf.expand_dims(image, 0), return_attention=True)\n",
    "    predicted_class = class_names[tf.argmax(predictions[0])]\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
    "    fig.suptitle(f'Attention Maps - Predicted: {predicted_class}', fontsize=16)\n",
    "    \n",
    "    # Show original image\n",
    "    axes[0, 1].imshow(image)\n",
    "    axes[0, 1].set_title('Original Image')\n",
    "    axes[0, 1].axis('off')\n",
    "    \n",
    "    # Show attention maps from different layers\n",
    "    for i, attn_weight in enumerate(attention_weights[:8]):\n",
    "        row = (i + 1) // 3\n",
    "        col = (i + 1) % 3\n",
    "        \n",
    "        # Average attention across heads\n",
    "        attn_map = tf.reduce_mean(attn_weight[0], axis=0)\n",
    "        \n",
    "        # Reshape to 2D grid\n",
    "        grid_size = int(np.sqrt(attn_map.shape[0]))\n",
    "        attn_map_2d = tf.reshape(attn_map, (grid_size, grid_size))\n",
    "        \n",
    "        # Resize to image size\n",
    "        attn_map_resized = tf.image.resize(\n",
    "            tf.expand_dims(attn_map_2d, -1),\n",
    "            (image.shape[0], image.shape[1])\n",
    "        )\n",
    "        \n",
    "        # Display\n",
    "        im = axes[row, col].imshow(attn_map_resized[:, :, 0], cmap='hot', alpha=0.7)\n",
    "        axes[row, col].imshow(image, alpha=0.3)\n",
    "        axes[row, col].set_title(f'Layer {i+1}')\n",
    "        axes[row, col].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5af233e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 87\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# Usage\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 87\u001b[0m     experiment, model, results \u001b[38;5;241m=\u001b[39m train_enhanced_model(\n\u001b[0;32m     88\u001b[0m         meta_train_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmeta_train.csv\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     89\u001b[0m         train_images_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_images/\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     90\u001b[0m         test_images_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_images/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     91\u001b[0m     )\n",
      "Cell \u001b[1;32mIn[23], line 7\u001b[0m, in \u001b[0;36mtrain_enhanced_model\u001b[1;34m(meta_train_path, train_images_path, test_images_path)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Main training pipeline with all HD/DI improvements\"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Split data into train and test sets\u001b[39;00m\n\u001b[0;32m      6\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[1;32m----> 7\u001b[0m     X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, stratify\u001b[38;5;241m=\u001b[39my, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[0;32m      8\u001b[0m )\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Initialize experiment manager\u001b[39;00m\n\u001b[0;32m     11\u001b[0m experiment \u001b[38;5;241m=\u001b[39m ExperimentManager(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menhanced_vit_task1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "# **IMPROVEMENT 10: Main training pipeline with all enhancements**\n",
    "def train_enhanced_model(meta_train_path, train_images_path, test_images_path):\n",
    "    \"\"\"Main training pipeline with all HD/DI improvements\"\"\"\n",
    "    \n",
    "    # Split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, stratify=y, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Initialize experiment manager\n",
    "    experiment = ExperimentManager(\"enhanced_vit_task1\")\n",
    "    \n",
    "    # Load metadata\n",
    "    meta_df = pd.read_csv(meta_train_path)\n",
    "    \n",
    "    # Comprehensive dataset analysis\n",
    "    logger.info(\"Performing comprehensive dataset analysis...\")\n",
    "    dataset_analysis = analyze_dataset_comprehensive(meta_df)\n",
    "    \n",
    "    # Create balanced data generators\n",
    "    logger.info(\"Creating balanced data generators...\")\n",
    "    data_gen, class_weights = create_balanced_data_generator(meta_df)\n",
    "    \n",
    "    # Train with cross-validation\n",
    "    logger.info(\"Starting stratified cross-validation...\")\n",
    "    cv_results = stratified_cross_validation(X_train, y_train)\n",
    "    \n",
    "    # Create ensemble model\n",
    "    logger.info(\"Creating ensemble model...\")\n",
    "    ensemble_models = []\n",
    "    for fold in range(5):\n",
    "        model = tf.keras.models.load_model(f'models/fold_{fold}_best_model.keras')\n",
    "        ensemble_models.append(model)\n",
    "    \n",
    "    ensemble = ModelEnsemble(ensemble_models)\n",
    "    \n",
    "    # Comprehensive evaluation\n",
    "    logger.info(\"Performing comprehensive evaluation...\")\n",
    "    evaluation_results = comprehensive_model_evaluation(\n",
    "        ensemble,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        class_names=list(meta_df['label'].unique())\n",
    "    )\n",
    "    \n",
    "    # Generate attention visualizations\n",
    "    logger.info(\"Generating attention visualizations...\")\n",
    "    sample_indices = np.random.choice(len(X_test), 5, replace=False)\n",
    "    for i, idx in enumerate(sample_indices):\n",
    "        visualize_attention_maps(\n",
    "            ensemble_models[0],  # Use first model for attention\n",
    "            X_test[idx],\n",
    "            list(meta_df['label'].unique()),\n",
    "            f'attention_visualization_{i}.png'\n",
    "        )\n",
    "    \n",
    "    # Save final results\n",
    "    experiment.log_experiment(\n",
    "        \"enhanced_ensemble_vit\",\n",
    "        evaluation_results,\n",
    "        {\n",
    "            'architecture': 'Enhanced ViT with ensemble',\n",
    "            'cross_validation_folds': 5,\n",
    "            'augmentation': 'Advanced class-balanced augmentation',\n",
    "            'class_weights': class_weights\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Generate final predictions for submission\n",
    "    logger.info(\"Generating final predictions...\")\n",
    "    test_predictions = ensemble.predict_proba(test_images)\n",
    "    predicted_labels = [class_names[np.argmax(pred)] for pred in test_predictions]\n",
    "    \n",
    "    # Create submission file\n",
    "    submission_df = pd.DataFrame({\n",
    "        'image_id': [f.split('/')[-1] for f in test_files],\n",
    "        'label': predicted_labels,\n",
    "        'confidence': [np.max(pred) for pred in test_predictions]\n",
    "    })\n",
    "    \n",
    "    submission_df.to_csv('enhanced_submission.csv', index=False)\n",
    "    \n",
    "    return experiment, ensemble, evaluation_results\n",
    "\n",
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    experiment, model, results = train_enhanced_model(\n",
    "        meta_train_path='meta_train.csv',\n",
    "        train_images_path='train_images/',\n",
    "        test_images_path='test_images/'\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
