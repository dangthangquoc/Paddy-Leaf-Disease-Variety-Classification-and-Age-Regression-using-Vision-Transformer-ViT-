{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae886618",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Sequential, Model, Input\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import warnings\n",
    "import logging\n",
    "import json\n",
    "from glob import glob\n",
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "random.seed(45)\n",
    "np.random.seed(45)\n",
    "tf.random.set_seed(45)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# **DEFINE MISSING CLASSES AND FUNCTIONS**\n",
    "\n",
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super(Patches, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "        return patches\n",
    "\n",
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super(PatchEncoder, self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, name=None):\n",
    "        super(TransformerBlock, self).__init__(name=name)\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential([\n",
    "            layers.Dense(ff_dim, activation=\"gelu\"),\n",
    "            layers.Dense(embed_dim),\n",
    "        ])\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=None, return_attention=False):\n",
    "        attn_output, attn_weights = self.att(inputs, inputs, return_attention_scores=True)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "        \n",
    "        if return_attention:\n",
    "            return out2, attn_weights\n",
    "        return out2\n",
    "\n",
    "# **DATA LOADING FUNCTIONS**\n",
    "\n",
    "def load_images_and_labels(train_images_path, meta_df):\n",
    "    \"\"\"Load images and create labels array\"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "    image_paths = []\n",
    "    \n",
    "    logger.info(\"Loading training images...\")\n",
    "    for idx, row in tqdm(meta_df.iterrows(), total=len(meta_df)):\n",
    "        image_id = row['image_id']\n",
    "        label = row['label']\n",
    "        \n",
    "    \n",
    "        image_path = os.path.join(train_images_path, label, f\"{image_id}\")\n",
    "        \n",
    "        if os.path.exists(image_path):\n",
    "            # Load and preprocess image\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            image = image.resize((256, 256))\n",
    "            image_array = np.array(image) / 255.0\n",
    "            \n",
    "            images.append(image_array)\n",
    "            labels.append(label)\n",
    "            image_paths.append(image_path)\n",
    "    \n",
    "    # Convert labels to numeric\n",
    "    unique_labels = sorted(meta_df['label'].unique())\n",
    "    label_to_index = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "    numeric_labels = [label_to_index[label] for label in labels]\n",
    "    \n",
    "    return np.array(images), np.array(numeric_labels), unique_labels, image_paths\n",
    "\n",
    "def load_test_images(test_images_path):\n",
    "    \"\"\"Load test images for prediction\"\"\"\n",
    "    images = []\n",
    "    image_paths = []\n",
    "    \n",
    "    logger.info(\"Loading test images...\")\n",
    "    for image_file in tqdm(os.listdir(test_images_path)):\n",
    "        if image_file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "            image_path = os.path.join(test_images_path, image_file)\n",
    "            \n",
    "            # Load and preprocess image\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            image = image.resize((256, 256))\n",
    "            image_array = np.array(image) / 255.0\n",
    "            \n",
    "            images.append(image_array)\n",
    "            image_paths.append(image_file)\n",
    "    \n",
    "    return np.array(images), image_paths\n",
    "\n",
    "# **EXPERIMENT MANAGER CLASS**\n",
    "\n",
    "class ExperimentManager:\n",
    "    def __init__(self, experiment_name):\n",
    "        self.experiment_name = experiment_name\n",
    "        self.results_dir = f\"results/{experiment_name}\"\n",
    "        os.makedirs(self.results_dir, exist_ok=True)\n",
    "        os.makedirs(\"models\", exist_ok=True)\n",
    "        os.makedirs(\"model_evaluation\", exist_ok=True)\n",
    "        self.results = {}\n",
    "    \n",
    "    def log_experiment(self, model_name, metrics, config):\n",
    "        self.results[model_name] = {\n",
    "            'metrics': metrics,\n",
    "            'config': config\n",
    "        }\n",
    "        \n",
    "        # Save results to JSON\n",
    "        with open(f\"{self.results_dir}/results.json\", 'w') as f:\n",
    "            json.dump(self.results, f, indent=4)\n",
    "\n",
    "# **DATASET ANALYSIS**\n",
    "\n",
    "def analyze_dataset_comprehensive(meta_train_df):\n",
    "    \"\"\"Perform comprehensive dataset analysis for HD/DI requirements\"\"\"\n",
    "    \n",
    "    # Class imbalance analysis\n",
    "    class_distribution = meta_train_df['label'].value_counts()\n",
    "    imbalance_ratio = class_distribution.max() / class_distribution.min()\n",
    "    \n",
    "    # Create visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(20, 16))\n",
    "    \n",
    "    # Plot 1: Class distribution\n",
    "    class_distribution.plot(kind='bar', ax=axes[0,0])\n",
    "    axes[0,0].set_title(f'Class Distribution (Imbalance Ratio: {imbalance_ratio:.2f}:1)')\n",
    "    axes[0,0].set_xlabel('Disease Class')\n",
    "    axes[0,0].set_ylabel('Number of Images')\n",
    "    axes[0,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Plot 2: Variety distribution\n",
    "    variety_distribution = meta_train_df['variety'].value_counts()\n",
    "    variety_distribution.plot(kind='bar', ax=axes[0,1])\n",
    "    axes[0,1].set_title('Variety Distribution')\n",
    "    axes[0,1].set_xlabel('Paddy Variety')\n",
    "    axes[0,1].set_ylabel('Number of Images')\n",
    "    axes[0,1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Plot 3: Age distribution\n",
    "    axes[1,0].hist(meta_train_df['age'], bins=30, edgecolor='black')\n",
    "    axes[1,0].set_title('Age Distribution')\n",
    "    axes[1,0].set_xlabel('Age (days)')\n",
    "    axes[1,0].set_ylabel('Frequency')\n",
    "    \n",
    "    # Plot 4: Class-Variety heatmap\n",
    "    cross_table = pd.crosstab(meta_train_df['label'], meta_train_df['variety'])\n",
    "    sns.heatmap(cross_table, cmap='YlOrRd', annot=True, fmt='d', ax=axes[1,1])\n",
    "    axes[1,1].set_title('Class-Variety Distribution Heatmap')\n",
    "    axes[1,1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('dataset_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    return {\n",
    "        'class_distribution': class_distribution,\n",
    "        'imbalance_ratio': imbalance_ratio,\n",
    "        'variety_distribution': variety_distribution,\n",
    "        'age_stats': {\n",
    "            'mean': meta_train_df['age'].mean(),\n",
    "            'std': meta_train_df['age'].std(),\n",
    "            'min': meta_train_df['age'].min(),\n",
    "            'max': meta_train_df['age'].max()\n",
    "        }\n",
    "    }\n",
    "\n",
    "# **ENHANCED VIT MODEL**\n",
    "\n",
    "def create_enhanced_vit_classifier(num_classes, input_shape=(256, 256, 3)):\n",
    "    \"\"\"Create enhanced ViT classifier with better architecture\"\"\"\n",
    "    \n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Data augmentation\n",
    "    data_augmentation = keras.Sequential([\n",
    "        layers.Rescaling(1.0/255.0),\n",
    "        layers.Resizing(72, 72),\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(factor=0.05),\n",
    "        layers.RandomZoom(height_factor=0.3, width_factor=0.3),\n",
    "        layers.RandomContrast(factor=0.2),\n",
    "        layers.RandomBrightness(factor=0.2)\n",
    "    ])\n",
    "    \n",
    "    x = data_augmentation(inputs)\n",
    "    \n",
    "    # Create patches\n",
    "    patches = Patches(patch_size=6)(x)\n",
    "    \n",
    "    # Encode patches\n",
    "    encoded_patches = PatchEncoder(num_patches=144, projection_dim=64)(patches)\n",
    "    \n",
    "    # Transformer blocks\n",
    "    for i in range(8):\n",
    "        encoded_patches = TransformerBlock(\n",
    "            embed_dim=64,\n",
    "            num_heads=4,\n",
    "            ff_dim=128,\n",
    "            rate=0.1\n",
    "        )(encoded_patches)\n",
    "    \n",
    "    # Classification head\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    \n",
    "    # Enhanced MLP head\n",
    "    x = layers.Dense(2048, activation='gelu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Dense(1024, activation='gelu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Dense(512, activation='gelu')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    outputs = layers.Dense(num_classes)(x)\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# **CROSS VALIDATION**\n",
    "\n",
    "def stratified_cross_validation(X, y, num_classes, n_splits=5):\n",
    "    \"\"\"Perform stratified k-fold cross-validation\"\"\"\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    cv_results = []\n",
    "    fold_models = []\n",
    "    \n",
    "    # Calculate class weights for the entire dataset\n",
    "    class_weights = compute_class_weight(\n",
    "        'balanced',\n",
    "        classes=np.unique(y),\n",
    "        y=y\n",
    "    )\n",
    "    class_weight_dict = dict(enumerate(class_weights))\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "        logger.info(f\"Training fold {fold + 1}/{n_splits}\")\n",
    "        \n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        # Create and compile model for this fold\n",
    "        model = create_enhanced_vit_classifier(num_classes=num_classes)\n",
    "        \n",
    "        optimizer = keras.optimizers.AdamW(\n",
    "            learning_rate=0.001,\n",
    "            weight_decay=0.0001\n",
    "        )\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        # Define callbacks\n",
    "        callbacks = [\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                monitor='val_accuracy',\n",
    "                patience=15,\n",
    "                restore_best_weights=True,\n",
    "                verbose=1\n",
    "            ),\n",
    "            tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                monitor='val_accuracy',\n",
    "                factor=0.5,\n",
    "                patience=7,\n",
    "                min_lr=1e-7,\n",
    "                verbose=1\n",
    "            ),\n",
    "            tf.keras.callbacks.ModelCheckpoint(\n",
    "                f'models/fold_{fold}_best_model.keras',\n",
    "                monitor='val_accuracy',\n",
    "                save_best_only=True,\n",
    "                verbose=1\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=100,\n",
    "            batch_size=32,\n",
    "            callbacks=callbacks,\n",
    "            class_weight=class_weight_dict,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Evaluate fold\n",
    "        val_loss, val_accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
    "        \n",
    "        # Store results\n",
    "        fold_results = {\n",
    "            'fold': fold + 1,\n",
    "            'best_val_accuracy': max(history.history['val_accuracy']),\n",
    "            'final_val_accuracy': val_accuracy\n",
    "        }\n",
    "        \n",
    "        cv_results.append(fold_results)\n",
    "        fold_models.append(model)\n",
    "        \n",
    "        logger.info(f\"Fold {fold + 1} completed - Val Accuracy: {val_accuracy:.4f}\")\n",
    "    \n",
    "    return cv_results, fold_models\n",
    "\n",
    "# **MODEL ENSEMBLE**\n",
    "\n",
    "class ModelEnsemble:\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "    \n",
    "    def predict(self, X, batch_size=32):\n",
    "        \"\"\"Ensemble prediction using voting\"\"\"\n",
    "        predictions = []\n",
    "        \n",
    "        for model in self.models:\n",
    "            pred = model.predict(X, batch_size=batch_size)\n",
    "            predictions.append(pred)\n",
    "        \n",
    "        # Average logits (before softmax)\n",
    "        ensemble_pred = np.mean(predictions, axis=0)\n",
    "        return ensemble_pred\n",
    "    \n",
    "    def predict_proba(self, X, batch_size=32):\n",
    "        \"\"\"Get probability predictions from ensemble\"\"\"\n",
    "        predictions = []\n",
    "        \n",
    "        for model in self.models:\n",
    "            pred = model.predict(X, batch_size=batch_size)\n",
    "            predictions.append(tf.nn.softmax(pred).numpy())\n",
    "        \n",
    "        # Average probabilities\n",
    "        ensemble_proba = np.mean(predictions, axis=0)\n",
    "        return ensemble_proba\n",
    "\n",
    "# **COMPREHENSIVE EVALUATION**\n",
    "\n",
    "def comprehensive_model_evaluation(model, X_test, y_test, class_names):\n",
    "    \"\"\"Comprehensive evaluation with advanced metrics and visualizations\"\"\"\n",
    "    \n",
    "    # Get predictions\n",
    "    y_pred_logits = model.predict(X_test)\n",
    "    y_pred_probs = tf.nn.softmax(y_pred_logits).numpy()\n",
    "    y_pred = np.argmax(y_pred_logits, axis=1)\n",
    "    \n",
    "    # Create evaluation directory\n",
    "    eval_dir = \"model_evaluation\"\n",
    "    os.makedirs(eval_dir, exist_ok=True)\n",
    "    \n",
    "    # 1. Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{eval_dir}/confusion_matrix.png', dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Per-class metrics\n",
    "    report = classification_report(y_test, y_pred, target_names=class_names, output_dict=True)\n",
    "    \n",
    "    # Save classification report\n",
    "    with open(f'{eval_dir}/classification_report.txt', 'w') as f:\n",
    "        f.write(classification_report(y_test, y_pred, target_names=class_names))\n",
    "    \n",
    "    # Visualize per-class metrics\n",
    "    metrics_df = pd.DataFrame(report).transpose()\n",
    "    metrics_df = metrics_df.drop(['accuracy', 'macro avg', 'weighted avg'])\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    metrics_df[['precision', 'recall', 'f1-score']].plot(kind='bar', ax=ax)\n",
    "    plt.title('Per-Class Performance Metrics')\n",
    "    plt.xlabel('Disease Class')\n",
    "    plt.ylabel('Score')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{eval_dir}/per_class_metrics.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. ROC curves for each class\n",
    "    y_test_bin = label_binarize(y_test, classes=range(len(class_names)))\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    colors = plt.cm.rainbow(np.linspace(0, 1, len(class_names)))\n",
    "    \n",
    "    for i, color in zip(range(len(class_names)), colors):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_pred_probs[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "                label=f'{class_names[i]} (AUC = {roc_auc[i]:.3f})')\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curves for Each Class')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{eval_dir}/roc_curves.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    return {\n",
    "        'confusion_matrix': cm,\n",
    "        'classification_report': report,\n",
    "        'roc_auc': roc_auc\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7deab505",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading metadata...\n",
      "INFO:__main__:Performing comprehensive dataset analysis...\n",
      "INFO:__main__:Loading training data...\n",
      "INFO:__main__:Loading training images...\n",
      "100%|██████████| 10407/10407 [05:38<00:00, 30.72it/s]\n",
      "INFO:__main__:Loaded 10407 training images with 10 classes\n",
      "INFO:__main__:Class names: ['bacterial_leaf_blight', 'bacterial_leaf_streak', 'bacterial_panicle_blight', 'blast', 'brown_spot', 'dead_heart', 'downy_mildew', 'hispa', 'normal', 'tungro']\n",
      "INFO:__main__:Training set: 8325 images\n",
      "INFO:__main__:Test set: 2082 images\n",
      "INFO:__main__:Starting stratified cross-validation...\n",
      "INFO:__main__:Training fold 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ThinkPad\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:184: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ThinkPad\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:184: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 4.88 GiB for an array with shape (6660, 256, 256, 3) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 92\u001b[0m\n\u001b[0;32m     89\u001b[0m HOME_PATH \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetcwd() \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# Run the complete training pipeline\u001b[39;00m\n\u001b[1;32m---> 92\u001b[0m experiment, model, results \u001b[38;5;241m=\u001b[39m train_enhanced_model(\n\u001b[0;32m     93\u001b[0m     meta_train_path\u001b[38;5;241m=\u001b[39m HOME_PATH \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmeta_train.csv\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     94\u001b[0m     train_images_path\u001b[38;5;241m=\u001b[39m HOME_PATH \u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_images/\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     95\u001b[0m     test_images_path\u001b[38;5;241m=\u001b[39m HOME_PATH \u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_images/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     96\u001b[0m )\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# Print summary\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m)\n",
      "Cell \u001b[1;32mIn[25], line 34\u001b[0m, in \u001b[0;36mtrain_enhanced_model\u001b[1;34m(meta_train_path, train_images_path, test_images_path)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Train with cross-validation\u001b[39;00m\n\u001b[0;32m     33\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting stratified cross-validation...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 34\u001b[0m cv_results, fold_models \u001b[38;5;241m=\u001b[39m stratified_cross_validation(\n\u001b[0;32m     35\u001b[0m     X_train, y_train, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(class_names), n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m\n\u001b[0;32m     36\u001b[0m )\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Create ensemble model\u001b[39;00m\n\u001b[0;32m     39\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating ensemble model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[24], line 337\u001b[0m, in \u001b[0;36mstratified_cross_validation\u001b[1;34m(X, y, num_classes, n_splits)\u001b[0m\n\u001b[0;32m    314\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    315\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(\n\u001b[0;32m    316\u001b[0m         monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    333\u001b[0m     )\n\u001b[0;32m    334\u001b[0m ]\n\u001b[0;32m    336\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m    338\u001b[0m     X_train, y_train,\n\u001b[0;32m    339\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39m(X_val, y_val),\n\u001b[0;32m    340\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[0;32m    341\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[0;32m    342\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[0;32m    343\u001b[0m     class_weight\u001b[38;5;241m=\u001b[39mclass_weight_dict,\n\u001b[0;32m    344\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    345\u001b[0m )\n\u001b[0;32m    347\u001b[0m \u001b[38;5;66;03m# Evaluate fold\u001b[39;00m\n\u001b[0;32m    348\u001b[0m val_loss, val_accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_val, y_val, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ThinkPad\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\ThinkPad\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:96\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Converts the given `value` to an `EagerTensor`.\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \n\u001b[0;32m     78\u001b[0m \u001b[38;5;124;03mNote that this function could return cached copies of created constants for\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;124;03m  TypeError: if `dtype` is not compatible with the type of t.\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m     93\u001b[0m   \u001b[38;5;66;03m# Make a copy explicitly because the EagerTensor might share the underlying\u001b[39;00m\n\u001b[0;32m     94\u001b[0m   \u001b[38;5;66;03m# memory with the input array. Without this copy, users will be able to\u001b[39;00m\n\u001b[0;32m     95\u001b[0m   \u001b[38;5;66;03m# modify the EagerTensor after its creation by changing the input array.\u001b[39;00m\n\u001b[1;32m---> 96\u001b[0m   value \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, ops\u001b[38;5;241m.\u001b[39mEagerTensor):\n\u001b[0;32m     98\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m dtype:\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 4.88 GiB for an array with shape (6660, 256, 256, 3) and data type float32"
     ]
    }
   ],
   "source": [
    "# **MAIN TRAINING PIPELINE**\n",
    "\n",
    "def train_enhanced_model(meta_train_path, train_images_path, test_images_path):\n",
    "    \"\"\"Main training pipeline with all HD/DI improvements\"\"\"\n",
    "    \n",
    "    # Initialize experiment manager\n",
    "    experiment = ExperimentManager(\"enhanced_vit_task1\")\n",
    "    \n",
    "    # Load metadata\n",
    "    logger.info(\"Loading metadata...\")\n",
    "    meta_df = pd.read_csv(meta_train_path)\n",
    "    \n",
    "    # Comprehensive dataset analysis\n",
    "    logger.info(\"Performing comprehensive dataset analysis...\")\n",
    "    dataset_analysis = analyze_dataset_comprehensive(meta_df)\n",
    "    \n",
    "    # Load images and labels\n",
    "    logger.info(\"Loading training data...\")\n",
    "    X, y, class_names, image_paths = load_images_and_labels(train_images_path, meta_df)\n",
    "    \n",
    "    logger.info(f\"Loaded {len(X)} training images with {len(class_names)} classes\")\n",
    "    logger.info(f\"Class names: {class_names}\")\n",
    "    \n",
    "    # Split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, stratify=y, random_state=42\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"Training set: {len(X_train)} images\")\n",
    "    logger.info(f\"Test set: {len(X_test)} images\")\n",
    "    \n",
    "    # Train with cross-validation\n",
    "    logger.info(\"Starting stratified cross-validation...\")\n",
    "    cv_results, fold_models = stratified_cross_validation(\n",
    "        X_train, y_train, num_classes=len(class_names), n_splits=5\n",
    "    )\n",
    "    \n",
    "    # Create ensemble model\n",
    "    logger.info(\"Creating ensemble model...\")\n",
    "    ensemble = ModelEnsemble(fold_models)\n",
    "    \n",
    "    # Comprehensive evaluation on test set\n",
    "    logger.info(\"Performing comprehensive evaluation...\")\n",
    "    evaluation_results = comprehensive_model_evaluation(\n",
    "        ensemble, X_test, y_test, class_names\n",
    "    )\n",
    "    \n",
    "    # Load test images\n",
    "    logger.info(\"Loading test images for final prediction...\")\n",
    "    test_images, test_image_files = load_test_images(test_images_path)\n",
    "    \n",
    "    # Generate final predictions for submission\n",
    "    logger.info(\"Generating final predictions...\")\n",
    "    test_predictions = ensemble.predict_proba(test_images)\n",
    "    predicted_labels = [class_names[np.argmax(pred)] for pred in test_predictions]\n",
    "    confidence_scores = [np.max(pred) for pred in test_predictions]\n",
    "    \n",
    "    # Create submission file\n",
    "    submission_df = pd.DataFrame({\n",
    "        'image_id': test_image_files,\n",
    "        'label': predicted_labels,\n",
    "        'confidence': confidence_scores\n",
    "    })\n",
    "    \n",
    "    submission_df.to_csv('enhanced_submission.csv', index=False)\n",
    "    logger.info(\"Submission file created: enhanced_submission.csv\")\n",
    "    \n",
    "    # Save experiment results\n",
    "    experiment.log_experiment(\n",
    "        \"enhanced_ensemble_vit\",\n",
    "        evaluation_results,\n",
    "        {\n",
    "            'architecture': 'Enhanced ViT with ensemble',\n",
    "            'cross_validation_folds': 5,\n",
    "            'class_weights': 'balanced',\n",
    "            'augmentation': 'Advanced data augmentation',\n",
    "            'num_classes': len(class_names),\n",
    "            'cv_results': cv_results\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    logger.info(\"Training pipeline completed successfully!\")\n",
    "    \n",
    "    return experiment, ensemble, evaluation_results\n",
    "\n",
    "# **MAIN EXECUTION**\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    HOME_PATH = os.getcwd() + \"/\"\n",
    "    \n",
    "    # Run the complete training pipeline\n",
    "    experiment, model, results = train_enhanced_model(\n",
    "        meta_train_path= HOME_PATH + 'meta_train.csv',\n",
    "        train_images_path= HOME_PATH +'train_images/',\n",
    "        test_images_path= HOME_PATH +'test_images/'\n",
    "    )\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Results saved in: {experiment.results_dir}\")\n",
    "    print(f\"Models saved in: models/\")\n",
    "    print(f\"Evaluation plots saved in: model_evaluation/\")\n",
    "    print(f\"Final submission saved as: enhanced_submission.csv\")\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5777d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_train_path= HOME_PATH + 'meta_train.csv',\n",
    "train_images_path= HOME_PATH +'train_images/',\n",
    "test_images_path= HOME_PATH +'test_images/'\n",
    "\n",
    "# Initialize experiment manager\n",
    "experiment = ExperimentManager(\"enhanced_vit_task1\")\n",
    "\n",
    "# Load metadata\n",
    "logger.info(\"Loading metadata...\")\n",
    "meta_df = pd.read_csv(meta_train_path)\n",
    "\n",
    "# Comprehensive dataset analysis\n",
    "logger.info(\"Performing comprehensive dataset analysis...\")\n",
    "dataset_analysis = analyze_dataset_comprehensive(meta_df)\n",
    "\n",
    "# Load images and labels\n",
    "logger.info(\"Loading training data...\")\n",
    "X, y, class_names, image_paths = load_images_and_labels(train_images_path, meta_df)\n",
    "\n",
    "logger.info(f\"Loaded {len(X)} training images with {len(class_names)} classes\")\n",
    "logger.info(f\"Class names: {class_names}\")\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "logger.info(f\"Training set: {len(X_train)} images\")\n",
    "logger.info(f\"Test set: {len(X_test)} images\")\n",
    "\n",
    "# Train with cross-validation\n",
    "logger.info(\"Starting stratified cross-validation...\")\n",
    "cv_results, fold_models = stratified_cross_validation(\n",
    "    X_train, y_train, num_classes=len(class_names), n_splits=5\n",
    ")\n",
    "\n",
    "# Create ensemble model\n",
    "logger.info(\"Creating ensemble model...\")\n",
    "ensemble = ModelEnsemble(fold_models)\n",
    "\n",
    "# Comprehensive evaluation on test set\n",
    "logger.info(\"Performing comprehensive evaluation...\")\n",
    "evaluation_results = comprehensive_model_evaluation(\n",
    "    ensemble, X_test, y_test, class_names\n",
    ")\n",
    "\n",
    "# Load test images\n",
    "logger.info(\"Loading test images for final prediction...\")\n",
    "test_images, test_image_files = load_test_images(test_images_path)\n",
    "\n",
    "# Generate final predictions for submission\n",
    "logger.info(\"Generating final predictions...\")\n",
    "test_predictions = ensemble.predict_proba(test_images)\n",
    "predicted_labels = [class_names[np.argmax(pred)] for pred in test_predictions]\n",
    "confidence_scores = [np.max(pred) for pred in test_predictions]\n",
    "\n",
    "# Create submission file\n",
    "submission_df = pd.DataFrame({\n",
    "    'image_id': test_image_files,\n",
    "    'label': predicted_labels,\n",
    "    'confidence': confidence_scores\n",
    "})\n",
    "\n",
    "submission_df.to_csv('enhanced_submission.csv', index=False)\n",
    "logger.info(\"Submission file created: enhanced_submission.csv\")\n",
    "\n",
    "# Save experiment results\n",
    "experiment.log_experiment(\n",
    "    \"enhanced_ensemble_vit\",\n",
    "    evaluation_results,\n",
    "    {\n",
    "        'architecture': 'Enhanced ViT with ensemble',\n",
    "        'cross_validation_folds': 5,\n",
    "        'class_weights': 'balanced',\n",
    "        'augmentation': 'Advanced data augmentation',\n",
    "        'num_classes': len(class_names),\n",
    "        'cv_results': cv_results\n",
    "    }\n",
    ")\n",
    "\n",
    "logger.info(\"Training pipeline completed successfully!\")    \n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Results saved in: {experiment.results_dir}\")\n",
    "print(f\"Models saved in: models/\")\n",
    "print(f\"Evaluation plot     s saved in: model_evaluation/\")\n",
    "print(f\"Final submission saved as: enhanced_submission.csv\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
