{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be9c7bfe",
   "metadata": {},
   "source": [
    "# Task 1: Disease Classification Prediction Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b078284",
   "metadata": {},
   "source": [
    "# Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ffa007",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import warnings\n",
    "import joblib\n",
    "from PIL import Image, ImageFile\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Allow loading of truncated images\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3b420f",
   "metadata": {},
   "source": [
    "# Configuration constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011a68a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "img_height = 256\n",
    "img_width = 256\n",
    "\n",
    "# Define paths\n",
    "HOME_PATH = os.getcwd() + \"/\"\n",
    "TEST_IMG_PATH = HOME_PATH + 'test_images'\n",
    "LABEL_ENCODER_PATH = HOME_PATH + 'disease_label_encoder.joblib'\n",
    "WEIGHTS_PATH = HOME_PATH + 'paddy_models/vit_label_weights.weights.h5'\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "os.makedirs('paddy_models', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099d7045",
   "metadata": {},
   "source": [
    "# Load the label encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc0d1aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading label encoder...\n",
      "Number of unique labels: 10\n",
      "Labels: ['bacterial_leaf_blight' 'bacterial_leaf_streak'\n",
      " 'bacterial_panicle_blight' 'blast' 'brown_spot' 'dead_heart'\n",
      " 'downy_mildew' 'hispa' 'normal' 'tungro']\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading label encoder...\")\n",
    "label_encoder = joblib.load(LABEL_ENCODER_PATH)\n",
    "unique_labels = label_encoder.classes_\n",
    "num_labels = len(unique_labels)\n",
    "print(f\"Number of unique labels: {num_labels}\")\n",
    "print(f\"Labels: {unique_labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d39a93",
   "metadata": {},
   "source": [
    "# Define model architecture (must match the architecture used during training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2903365e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    \"\"\"Create a multi-layer perceptron with GELU activation and dropout.\"\"\"\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "\n",
    "# Patch extraction layer\n",
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super(Patches, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "        return patches\n",
    "\n",
    "# Patch encoding layer\n",
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super(PatchEncoder, self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df11fa5",
   "metadata": {},
   "source": [
    "# Function to create the Vision Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41c0a4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model architecture...\n",
      "WARNING:tensorflow:From c:\\Users\\ThinkPad\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:184: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def create_vit_label_classifier():\n",
    "    \"\"\"Create a Vision Transformer model for disease classification.\"\"\"\n",
    "    # ViT parameters (must match training configuration)\n",
    "    input_shape = (256, 256, 3)\n",
    "    image_size = 72\n",
    "    patch_size = 6\n",
    "    num_patches = (image_size // patch_size) ** 2\n",
    "    projection_dim = 64\n",
    "    num_heads = 4\n",
    "    transformer_units = [projection_dim * 2, projection_dim]\n",
    "    transformer_layers = 8\n",
    "    mlp_head_units = [2048, 1024]\n",
    "    \n",
    "    # Normalization layer\n",
    "    normalization = layers.Normalization()\n",
    "    \n",
    "    # Data augmentation layers (needed for model structure, but won't affect inference)\n",
    "    data_augmentation = keras.Sequential(\n",
    "        [\n",
    "            layers.Resizing(image_size, image_size),\n",
    "            layers.RandomFlip(\"horizontal\"),\n",
    "            layers.RandomRotation(factor=0.02),\n",
    "            layers.RandomZoom(height_factor=0.2, width_factor=0.2),\n",
    "        ],\n",
    "        name=\"data_augmentation\",\n",
    "    )\n",
    "    \n",
    "    # Model architecture definition\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    normalized = normalization(inputs)\n",
    "    augmented = data_augmentation(normalized)\n",
    "    patches = Patches(patch_size)(augmented)\n",
    "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "        )(x1, x1)\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Final layers for classification\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = layers.Flatten()(representation)\n",
    "    representation = layers.Dropout(0.5)(representation)\n",
    "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
    "    logits = layers.Dense(num_labels, activation='softmax', name='label_output')(features)\n",
    "    \n",
    "    # Create the model\n",
    "    model = keras.Model(inputs=inputs, outputs=logits)\n",
    "    return model\n",
    "\n",
    "# Create model with the same architecture\n",
    "print(\"Creating model architecture...\")\n",
    "vit_label_classifier = create_vit_label_classifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c688f1",
   "metadata": {},
   "source": [
    "# Load weights from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b6ef34a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model weights...\n",
      "Model weights loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading model weights...\")\n",
    "WEIGHTS_PATH = HOME_PATH + 'paddy_models/vit_label_weights.weights.h5'\n",
    "vit_label_classifier.load_weights(WEIGHTS_PATH)\n",
    "print(\"Model weights loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbd7ca5",
   "metadata": {},
   "source": [
    "# Get Dataset Images and Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c1a55db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating test dataset...\n"
     ]
    }
   ],
   "source": [
    "# Function to create a dataset for test images\n",
    "def create_test_dataset(test_path):\n",
    "    \"\"\"Create a TensorFlow dataset for test images (no labels).\"\"\"\n",
    "    test_files = []\n",
    "    test_ids = []\n",
    "    \n",
    "    # Collect paths and IDs of test images\n",
    "    for img_name in os.listdir(test_path):\n",
    "        if img_name.endswith('.jpg'):\n",
    "            img_path = os.path.join(test_path, img_name)\n",
    "            test_files.append(img_path)\n",
    "            test_ids.append(img_name)\n",
    "    \n",
    "    # Create dataset from test image paths\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(test_files)\n",
    "    dataset = dataset.map(lambda x: parse_image(x), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return dataset, test_ids\n",
    "\n",
    "# Function to parse and preprocess images\n",
    "def parse_image(file_path):\n",
    "    \"\"\"Load and preprocess an image from file path.\"\"\"\n",
    "    image = tf.io.read_file(file_path)\n",
    "    image = tf.io.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, [img_height, img_width])\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    return image\n",
    "\n",
    "# Create test dataset\n",
    "print(\"Creating test dataset...\")\n",
    "test_pred_dataset, test_image_ids = create_test_dataset(TEST_IMG_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ad613d",
   "metadata": {},
   "source": [
    "# Generate Predictions and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de2bd67b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 233ms/step\n",
      "Predictions saved to 'disease_predictions.csv'\n",
      "Detailed predictions saved to 'disease_predictions_detailed.csv'\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions\n",
    "print(\"Generating predictions...\")\n",
    "predictions = vit_label_classifier.predict(test_pred_dataset)\n",
    "predicted_label_indices = np.argmax(predictions, axis=1)\n",
    "predicted_labels = label_encoder.inverse_transform(predicted_label_indices)\n",
    "\n",
    "# Create submission dataframe\n",
    "submission_df = pd.DataFrame({\n",
    "    'image_id': test_image_ids,\n",
    "    'label': predicted_labels\n",
    "})\n",
    "\n",
    "# Save predictions\n",
    "submission_df.to_csv('disease_predictions.csv', index=False)\n",
    "print(\"Predictions saved to 'disease_predictions.csv'\")\n",
    "\n",
    "# Create a more detailed submission file with confidence scores\n",
    "confidence_df = pd.DataFrame({\n",
    "    'image_id': test_image_ids,\n",
    "    'label': predicted_labels,\n",
    "    'confidence': np.max(predictions, axis=1)\n",
    "})\n",
    "\n",
    "# Add top 3 predictions for each image\n",
    "for i in range(3):\n",
    "    top_n_indices = np.argsort(predictions, axis=1)[:, -(i+1)]\n",
    "    confidence_df[f'label_top_{i+1}'] = label_encoder.inverse_transform(top_n_indices)\n",
    "    confidence_df[f'confidence_top_{i+1}'] = np.sort(predictions, axis=1)[:, -(i+1)]\n",
    "\n",
    "confidence_df.to_csv('disease_predictions_detailed.csv', index=False)\n",
    "print(\"Detailed predictions saved to 'disease_predictions_detailed.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
