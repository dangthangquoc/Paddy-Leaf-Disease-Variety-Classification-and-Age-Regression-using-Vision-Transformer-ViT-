{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8934a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading metadata...\n",
      "Number of unique varieties: 10\n",
      "Varieties: ['ADT45' 'IR20' 'KarnatakaPonni' 'Onthanel' 'Ponni' 'Surya' 'Zonal'\n",
      " 'AndraPonni' 'AtchayaPonni' 'RR']\n",
      "Creating file DataFrame...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10407/10407 [00:00<00:00, 20676.86it/s]\n"
     ]
    }
   ],
   "source": [
    "# Memory-Efficient Version for Task 2: Variety Classification\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import cv2 as cv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageFile\n",
    "\n",
    "# Import TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, optimizers, losses, metrics, callbacks, Sequential, Model, Input\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "# Suppress TensorFlow warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(45)\n",
    "\n",
    "# Set Load Truncated Images to True\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "# Setup Configuration and Constants\n",
    "batch_size = 32  # Reduced batch size\n",
    "img_height = 256\n",
    "img_width = 256\n",
    "\n",
    "# Define paths\n",
    "HOME_PATH = os.getcwd() + \"/\"\n",
    "TRAIN_IMG_PATH = HOME_PATH + 'train_images'\n",
    "TEST_IMG_PATH = HOME_PATH + 'test_images'\n",
    "META_TRAIN_PATH = HOME_PATH + 'meta_train.csv'\n",
    "CHECKPOINT_MODEL_PATH = HOME_PATH + 'paddy_models/best_vit_variety_model.keras'\n",
    "FINAL_MODEL_PATH = HOME_PATH + 'paddy_models/vit_variety_model.keras'\n",
    "FINAL_WEIGHTS_PATH = HOME_PATH + 'paddy_models/vit_variety_weights.weights.h5'\n",
    "# Create models directory if it doesn't exist\n",
    "os.makedirs('paddy_models', exist_ok=True)\n",
    "\n",
    "# Load metadata\n",
    "print(\"Loading metadata...\")\n",
    "meta_train = pd.read_csv(META_TRAIN_PATH)\n",
    "\n",
    "# Check unique varieties\n",
    "unique_varieties = meta_train['variety'].unique()\n",
    "num_varieties = len(unique_varieties)\n",
    "print(f\"Number of unique varieties: {num_varieties}\")\n",
    "print(f\"Varieties: {unique_varieties}\")\n",
    "\n",
    "# Create variety label encoder\n",
    "variety_encoder = LabelEncoder()\n",
    "variety_labels = variety_encoder.fit_transform(meta_train['variety'])\n",
    "variety_to_idx = {variety: idx for idx, variety in enumerate(variety_encoder.classes_)}\n",
    "\n",
    "# Save label encoder for later use\n",
    "import joblib\n",
    "joblib.dump(variety_encoder, 'variety_label_encoder.joblib')\n",
    "\n",
    "# Create a DataFrame with file paths and variety labels\n",
    "def create_file_df(meta_df):\n",
    "    \"\"\"Create a DataFrame with file paths and variety indices\"\"\"\n",
    "    data = []\n",
    "    \n",
    "    for idx, row in tqdm(meta_df.iterrows(), total=len(meta_df)):\n",
    "        image_id = row['image_id']\n",
    "        variety = row['variety']\n",
    "        label = row['label']  # Disease label to find the folder\n",
    "        \n",
    "        # Construct image path\n",
    "        img_path = os.path.join(TRAIN_IMG_PATH, label, image_id)\n",
    "        \n",
    "        if os.path.exists(img_path):\n",
    "            data.append({\n",
    "                'file_path': img_path,\n",
    "                'variety_label': variety_to_idx[variety],\n",
    "                'variety_name': variety,\n",
    "                'image_id': image_id\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Create file DataFrame\n",
    "print(\"Creating file DataFrame...\")\n",
    "file_df = create_file_df(meta_train)\n",
    "# Split data\n",
    "train_df, val_df = train_test_split(\n",
    "    file_df, test_size=0.3, random_state=42, stratify=file_df['variety_label']\n",
    ")\n",
    "# Update parameters\n",
    "num_classes = num_varieties\n",
    "input_shape = (256, 256, 3)\n",
    "\n",
    "# Configure the hyperparameters\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.0001\n",
    "batch_size = 32\n",
    "num_epochs = 50\n",
    "image_size = 72 \n",
    "patch_size = 6 \n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "projection_dim = 64\n",
    "num_heads = 4\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]\n",
    "transformer_layers = 8\n",
    "mlp_head_units = [2048, 1024]\n",
    "# Create TensorFlow Dataset\n",
    "def parse_image(file_path, label):\n",
    "    \"\"\"Parse image from file path\"\"\"\n",
    "    image = tf.io.read_file(file_path)\n",
    "    image = tf.io.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, [img_height, img_width])\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    return image, label\n",
    "\n",
    "def create_dataset(df, batch_size=32, is_training=True):\n",
    "    \"\"\"Create TensorFlow dataset from DataFrame\"\"\"\n",
    "    file_paths = df['file_path'].values\n",
    "    labels = df['variety_label'].values\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((file_paths, labels))\n",
    "    dataset = dataset.map(parse_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    if is_training:\n",
    "        dataset = dataset.shuffle(buffer_size=1000)\n",
    "        dataset = dataset.repeat()  # Repeat dataset for multiple epochs\n",
    "    \n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return dataset\n",
    "# Create datasets\n",
    "train_dataset = create_dataset(train_df, batch_size=batch_size, is_training=True)\n",
    "val_dataset = create_dataset(val_df, batch_size=batch_size, is_training=False)\n",
    "\n",
    "# Calculate steps\n",
    "steps_per_epoch = len(train_df) // batch_size\n",
    "validation_steps = len(val_df) // batch_size\n",
    "# Data Augmentation\n",
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.Resizing(image_size, image_size),\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(factor=0.02),\n",
    "        layers.RandomZoom(\n",
    "            height_factor=0.2, width_factor=0.2\n",
    "        ),\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")\n",
    "\n",
    "# Normalization layer (will be adapted during training)\n",
    "normalization = layers.Normalization()\n",
    "\n",
    "# Adapt normalization on a sample of data\n",
    "sample_dataset = train_dataset.take(5)\n",
    "normalization.adapt(sample_dataset.map(lambda x, y: x))\n",
    "# Multilayer perceptron (MLP)\n",
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "\n",
    "# Implementing patch creation as a layer\n",
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super(Patches, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "        return patches\n",
    "\n",
    "# Implementing the patch encoding layer\n",
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super(PatchEncoder, self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded\n",
    "\n",
    "# ViT Model for Variety Classification\n",
    "def create_vit_variety_classifier():\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    # Normalize data\n",
    "    normalized = normalization(inputs)\n",
    "    # Augment data.\n",
    "    augmented = data_augmentation(normalized)\n",
    "    # Create patches.\n",
    "    patches = Patches(patch_size)(augmented)\n",
    "    # Encode patches.\n",
    "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "        )(x1, x1)\n",
    "        # Skip connection 1.\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "        # Skip connection 2.\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Create a [batch_size, projection_dim] tensor.\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = layers.Flatten()(representation)\n",
    "    representation = layers.Dropout(0.5)(representation)\n",
    "    # Add MLP.\n",
    "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
    "    # Classify outputs\n",
    "    logits = layers.Dense(num_classes, activation='softmax', name='variety_output')(features)\n",
    "    # Create the Keras model.\n",
    "    model = keras.Model(inputs=inputs, outputs=logits)\n",
    "    return model\n",
    "\n",
    "# Tạo hàm callback tùy chỉnh cho F1 Score\n",
    "class F1ScoreCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, validation_data, validation_steps):\n",
    "        super(F1ScoreCallback, self).__init__()\n",
    "        self.validation_data = validation_data\n",
    "        self.validation_steps = validation_steps\n",
    "        self.val_f1_scores = []\n",
    "        self.val_f1_scores_weighted = []\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Thu thập tất cả dự đoán và nhãn thực tế cho tập validation\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        \n",
    "        # Lấy dữ liệu từ dataset validation\n",
    "        for i in range(self.validation_steps):\n",
    "            x_batch, y_batch = next(iter(self.validation_data))\n",
    "            y_pred_batch = np.argmax(self.model.predict(x_batch), axis=1)\n",
    "            y_true.extend(y_batch.numpy())\n",
    "            y_pred.extend(y_pred_batch)\n",
    "        \n",
    "        # Tính F1 score\n",
    "        f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "        f1_weighted = f1_score(y_true, y_pred, average='weighted')\n",
    "        \n",
    "        # Lưu vào lịch sử\n",
    "        self.val_f1_scores.append(f1_macro)\n",
    "        self.val_f1_scores_weighted.append(f1_weighted)\n",
    "        \n",
    "        # In F1 score\n",
    "        print(f'\\nEpoch {epoch+1}: val_f1_macro = {f1_macro:.4f}, val_f1_weighted = {f1_weighted:.4f}')\n",
    "        \n",
    "        # Thêm vào logs để có thể theo dõi\n",
    "        logs['val_f1_macro'] = f1_macro\n",
    "        logs['val_f1_weighted'] = f1_weighted\n",
    "\n",
    "# Callbacks\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=CHECKPOINT_MODEL_PATH,\n",
    "    monitor=\"val_accuracy\",\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    mode='auto'\n",
    ")\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_accuracy\",\n",
    "    factor=0.1,\n",
    "    patience=10,\n",
    "    verbose=1,\n",
    "    mode=\"auto\",\n",
    "    min_delta=0.001,\n",
    "    cooldown=3,\n",
    "    min_lr=0\n",
    ")\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    patience=15,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Model Setup and Training\n",
    "def train_variety_model(model):\n",
    "    optimizer = keras.optimizers.AdamW(\n",
    "        learning_rate=learning_rate, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Tạo F1 score callback\n",
    "    f1_callback = F1ScoreCallback(val_dataset, validation_steps)\n",
    "\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        epochs=num_epochs,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        validation_data=val_dataset,\n",
    "        validation_steps=validation_steps,\n",
    "        callbacks=[checkpoint, reduce_lr, early_stopping, f1_callback],\n",
    "        verbose=1\n",
    "    )   \n",
    "    \n",
    "    # Thêm F1 score vào history\n",
    "    history.history['val_f1_macro'] = f1_callback.val_f1_scores\n",
    "    history.history['val_f1_weighted'] = f1_callback.val_f1_scores_weighted\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e42afb00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\conda\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:219: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create and train model\n",
    "vit_variety_classifier = create_vit_variety_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9813571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating test dataset...\n",
      "Generating predictions...\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 116ms/step\n",
      "Predictions saved to 'variety_predictions_with_f1.csv'\n",
      "Detailed predictions saved to 'variety_predictions_detailed_with_f1.csv'\n"
     ]
    }
   ],
   "source": [
    "# Generate Predictions for Test Set\n",
    "def create_test_dataset(test_path):\n",
    "    \"\"\"Create dataset for test images\"\"\"\n",
    "    test_files = []\n",
    "    test_ids = []\n",
    "    \n",
    "    for img_name in os.listdir(test_path):\n",
    "        if img_name.endswith('.jpg'):\n",
    "            img_path = os.path.join(test_path, img_name)\n",
    "            test_files.append(img_path)\n",
    "            test_ids.append(img_name)\n",
    "    \n",
    "    # Create dataset without labels\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(test_files)\n",
    "    dataset = dataset.map(lambda x: parse_image(x, 0)[0], num_parallel_calls=tf.data.AUTOTUNE)  # Only images, no labels\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return dataset, test_ids\n",
    "\n",
    "# Create test dataset\n",
    "print(\"Creating test dataset...\")\n",
    "test_pred_dataset, test_image_ids = create_test_dataset(TEST_IMG_PATH)\n",
    "\n",
    "# Generate predictions\n",
    "print(\"Generating predictions...\")\n",
    "predictions = vit_variety_classifier.predict(test_pred_dataset)\n",
    "predicted_variety_indices = np.argmax(predictions, axis=1)\n",
    "predicted_varieties = variety_encoder.inverse_transform(predicted_variety_indices)\n",
    "\n",
    "# Create submission dataframe\n",
    "submission_df = pd.DataFrame({\n",
    "    'image_id': test_image_ids,\n",
    "    'variety': predicted_varieties\n",
    "})\n",
    "\n",
    "# Save predictions\n",
    "submission_df.to_csv('variety_predictions_with_f1.csv', index=False)\n",
    "print(\"Predictions saved to 'variety_predictions_with_f1.csv'\")\n",
    "\n",
    "# Create a more detailed submission file\n",
    "confidence_df = pd.DataFrame({\n",
    "    'image_id': test_image_ids,\n",
    "    'variety': predicted_varieties,\n",
    "    'confidence': np.max(predictions, axis=1)\n",
    "})\n",
    "\n",
    "# Add top 3 predictions for each image\n",
    "for i in range(3):\n",
    "    top_n_indices = np.argsort(predictions, axis=1)[:, -(i+1)]\n",
    "    confidence_df[f'variety_top_{i+1}'] = variety_encoder.inverse_transform(top_n_indices)\n",
    "    confidence_df[f'confidence_top_{i+1}'] = np.sort(predictions, axis=1)[:, -(i+1)]\n",
    "\n",
    "confidence_df.to_csv('variety_predictions_detailed_with_f1.csv', index=False)\n",
    "print(\"Detailed predictions saved to 'variety_predictions_detailed_with_f1.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
