{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9269bbf1",
   "metadata": {},
   "source": [
    "# Task 2: Variety Classification Prediction Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8854724",
   "metadata": {},
   "source": [
    "# Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f76405a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import warnings\n",
    "import joblib\n",
    "from PIL import Image, ImageFile\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Allow loading of truncated images\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88988dcc",
   "metadata": {},
   "source": [
    "# Configuration constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db53690",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "img_height = 256\n",
    "img_width = 256\n",
    "\n",
    "# Define paths\n",
    "HOME_PATH = os.getcwd() + \"/\"\n",
    "TEST_IMG_PATH = HOME_PATH + 'test_images'\n",
    "VARIETY_ENCODER_PATH = HOME_PATH + 'encoder/variety_label_encoder.joblib'\n",
    "WEIGHTS_PATH = HOME_PATH + 'paddy_models/vit_variety_weights.weights.h5'\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "os.makedirs('paddy_models', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabf552c",
   "metadata": {},
   "source": [
    "# Load the variety encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a689be08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading variety encoder...\n",
      "Number of unique varieties: 10\n",
      "Varieties: ['ADT45' 'AndraPonni' 'AtchayaPonni' 'IR20' 'KarnatakaPonni' 'Onthanel'\n",
      " 'Ponni' 'RR' 'Surya' 'Zonal']\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading variety encoder...\")\n",
    "variety_encoder = joblib.load(VARIETY_ENCODER_PATH)\n",
    "unique_varieties = variety_encoder.classes_\n",
    "num_varieties = len(unique_varieties)\n",
    "print(f\"Number of unique varieties: {num_varieties}\")\n",
    "print(f\"Varieties: {unique_varieties}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7d5170",
   "metadata": {},
   "source": [
    "# Define model architecture & Patch extraction/encoding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c580bfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model architecture (must match the architecture used during training)\n",
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    \"\"\"Create a multi-layer perceptron with GELU activation and dropout.\"\"\"\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "\n",
    "# Patch extraction layer\n",
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super(Patches, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "        return patches\n",
    "\n",
    "# Patch encoding layer\n",
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super(PatchEncoder, self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b003b944",
   "metadata": {},
   "source": [
    "# Function to create the Vision Transformer model for variety classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f53f991a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vit_variety_classifier():\n",
    "    \"\"\"Create a Vision Transformer model for variety classification.\"\"\"\n",
    "    # ViT parameters (must match training configuration)\n",
    "    input_shape = (256, 256, 3)\n",
    "    image_size = 72\n",
    "    patch_size = 6\n",
    "    num_patches = (image_size // patch_size) ** 2\n",
    "    projection_dim = 64\n",
    "    num_heads = 4\n",
    "    transformer_units = [projection_dim * 2, projection_dim]\n",
    "    transformer_layers = 8\n",
    "    mlp_head_units = [2048, 1024]\n",
    "    \n",
    "    # Normalization layer\n",
    "    normalization = layers.Normalization()\n",
    "    \n",
    "    # Data augmentation layers (needed for model structure, but won't affect inference)\n",
    "    data_augmentation = keras.Sequential(\n",
    "        [\n",
    "            layers.Resizing(image_size, image_size),\n",
    "            layers.RandomFlip(\"horizontal\"),\n",
    "            layers.RandomRotation(factor=0.02),\n",
    "            layers.RandomZoom(height_factor=0.2, width_factor=0.2),\n",
    "        ],\n",
    "        name=\"data_augmentation\",\n",
    "    )\n",
    "    \n",
    "    # Model architecture definition\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    normalized = normalization(inputs)\n",
    "    augmented = data_augmentation(normalized)\n",
    "    patches = Patches(patch_size)(augmented)\n",
    "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "\n",
    "    # Create multiple layers of the Transformer block\n",
    "    for _ in range(transformer_layers):\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "        )(x1, x1)\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Final layers for classification\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = layers.Flatten()(representation)\n",
    "    representation = layers.Dropout(0.5)(representation)\n",
    "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
    "    logits = layers.Dense(num_varieties, activation='softmax', name='variety_output')(features)\n",
    "    \n",
    "    # Create the model\n",
    "    model = keras.Model(inputs=inputs, outputs=logits)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4de7f2",
   "metadata": {},
   "source": [
    "# Function to parse and process images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d347e10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_image(file_path):\n",
    "    \"\"\"Load and preprocess an image from file path.\"\"\"\n",
    "    image = tf.io.read_file(file_path)\n",
    "    image = tf.io.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, [img_height, img_width])\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    return image\n",
    "\n",
    "# Function to create a dataset for test images\n",
    "def create_test_dataset(test_path):\n",
    "    \"\"\"Create a TensorFlow dataset for test images (no labels).\"\"\"\n",
    "    test_files = []\n",
    "    test_ids = []\n",
    "    \n",
    "    # Collect paths and IDs of test images\n",
    "    for img_name in os.listdir(test_path):\n",
    "        if img_name.endswith('.jpg'):\n",
    "            img_path = os.path.join(test_path, img_name)\n",
    "            test_files.append(img_path)\n",
    "            test_ids.append(img_name)\n",
    "    \n",
    "    # Create dataset from test image paths\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(test_files)\n",
    "    dataset = dataset.map(lambda x: parse_image(x), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return dataset, test_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7317e656",
   "metadata": {},
   "source": [
    "# Load Model and Generate Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1273d81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model architecture...\n",
      "WARNING:tensorflow:From c:\\Users\\ThinkPad\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:184: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "Loading model weights...\n",
      "Model weights loaded successfully.\n",
      "Creating test dataset...\n",
      "Generating predictions...\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 192ms/step\n"
     ]
    }
   ],
   "source": [
    "# Create model with the same architecture\n",
    "print(\"Creating model architecture...\")\n",
    "vit_variety_classifier = create_vit_variety_classifier()\n",
    "\n",
    "# Load weights from file\n",
    "print(\"Loading model weights...\")\n",
    "vit_variety_classifier.load_weights(WEIGHTS_PATH)\n",
    "print(\"Model weights loaded successfully.\")\n",
    "\n",
    "# Create test dataset\n",
    "print(\"Creating test dataset...\")\n",
    "test_pred_dataset, test_image_ids = create_test_dataset(TEST_IMG_PATH)\n",
    "\n",
    "# Generate predictions\n",
    "print(\"Generating predictions...\")\n",
    "predictions = vit_variety_classifier.predict(test_pred_dataset)\n",
    "predicted_variety_indices = np.argmax(predictions, axis=1)\n",
    "predicted_varieties = variety_encoder.inverse_transform(predicted_variety_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cb44c2",
   "metadata": {},
   "source": [
    "# Save Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ca47650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to 'variety_predictions.csv'\n",
      "Detailed predictions saved to 'variety_predictions_detailed.csv'\n"
     ]
    }
   ],
   "source": [
    "# Create submission dataframe\n",
    "submission_df = pd.DataFrame({\n",
    "    'image_id': test_image_ids,\n",
    "    'variety': predicted_varieties\n",
    "})\n",
    "\n",
    "# Save predictions\n",
    "submission_df.to_csv('variety_predictions.csv', index=False)\n",
    "print(\"Predictions saved to 'variety_predictions.csv'\")\n",
    "\n",
    "# Create a more detailed submission file with confidence scores\n",
    "confidence_df = pd.DataFrame({\n",
    "    'image_id': test_image_ids,\n",
    "    'variety': predicted_varieties,\n",
    "    'confidence': np.max(predictions, axis=1)\n",
    "})\n",
    "\n",
    "# Add top 3 predictions for each image\n",
    "for i in range(3):\n",
    "    top_n_indices = np.argsort(predictions, axis=1)[:, -(i+1)]\n",
    "    confidence_df[f'variety_top_{i+1}'] = variety_encoder.inverse_transform(top_n_indices)\n",
    "    confidence_df[f'confidence_top_{i+1}'] = np.sort(predictions, axis=1)[:, -(i+1)]\n",
    "\n",
    "confidence_df.to_csv('variety_predictions_detailed.csv', index=False)\n",
    "print(\"Detailed predictions saved to 'variety_predictions_detailed.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
