{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "346f8dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading metadata...\n",
      "Number of unique varieties: 10\n",
      "Varieties: ['ADT45' 'AndraPonni' 'AtchayaPonni' 'IR20' 'KarnatakaPonni' 'Onthanel'\n",
      " 'Ponni' 'RR' 'Surya' 'Zonal']\n",
      "Creating file DataFrame...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10407/10407 [00:00<00:00, 23001.36it/s]\n"
     ]
    }
   ],
   "source": [
    "# Memory-Efficient Version for Task 2: Variety Classification\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, optimizers, losses, metrics, callbacks, Sequential, Model\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import warnings\n",
    "\n",
    "# Suppress TensorFlow warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Enable eager execution\n",
    "tf.config.run_functions_eagerly(True)\n",
    "if not tf.executing_eagerly():\n",
    "    raise RuntimeError(\"Eager execution is not enabled. Ensure TensorFlow 2.x is installed.\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(45)\n",
    "\n",
    "# Setup Configuration and Constants\n",
    "batch_size = 32\n",
    "img_height = 256\n",
    "img_width = 256\n",
    "num_epochs = 50\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.0001\n",
    "image_size = 72\n",
    "patch_size = 6\n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "projection_dim = 64\n",
    "num_heads = 4\n",
    "transformer_units = [projection_dim * 2, projection_dim]\n",
    "transformer_layers = 8\n",
    "mlp_head_units = [2048, 1024]\n",
    "n_splits = 4\n",
    "\n",
    "# Define paths\n",
    "HOME_PATH = os.getcwd() + \"/\"\n",
    "TRAIN_IMG_PATH = HOME_PATH + 'train_images'\n",
    "TEST_IMG_PATH = HOME_PATH + 'test_images'\n",
    "META_TRAIN_PATH = HOME_PATH + 'meta_train.csv'\n",
    "CHECKPOINT_MODEL_PATH = HOME_PATH + 'paddy_models/best_vit_variety_model_fold_{fold}.keras'\n",
    "\n",
    "# Create models directory\n",
    "os.makedirs('paddy_models', exist_ok=True)\n",
    "\n",
    "# Load metadata\n",
    "print(\"Loading metadata...\")\n",
    "meta_train = pd.read_csv(META_TRAIN_PATH)\n",
    "variety_encoder = LabelEncoder()\n",
    "variety_labels = variety_encoder.fit_transform(meta_train['variety'])\n",
    "variety_to_idx = {variety: idx for idx, variety in enumerate(variety_encoder.classes_)}\n",
    "num_varieties = len(variety_encoder.classes_)\n",
    "print(f\"Number of unique varieties: {num_varieties}\")\n",
    "print(f\"Varieties: {variety_encoder.classes_}\")\n",
    "joblib.dump(variety_encoder, 'variety_label_encoder.joblib')\n",
    "\n",
    "# Create DataFrame with file paths and variety labels\n",
    "def create_file_df(meta_df):\n",
    "    data = []\n",
    "    for idx, row in tqdm(meta_df.iterrows(), total=len(meta_df)):\n",
    "        image_id = row['image_id']\n",
    "        variety = row['variety']\n",
    "        label = row['label']\n",
    "        img_path = os.path.join(TRAIN_IMG_PATH, label, image_id)\n",
    "        if os.path.exists(img_path):\n",
    "            data.append({\n",
    "                'file_path': img_path,\n",
    "                'variety_label': variety_to_idx[variety],\n",
    "                'variety_name': variety,\n",
    "                'image_id': image_id\n",
    "            })\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "print(\"Creating file DataFrame...\")\n",
    "file_df = create_file_df(meta_train)\n",
    "\n",
    "# Image parsing function\n",
    "def parse_image(file_path, label):\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, [img_height, img_width])\n",
    "    img = img / 255.0\n",
    "    return img, label\n",
    "\n",
    "# Create dataset from DataFrame\n",
    "def create_dataset_from_df(df, batch_size=32, is_training=True):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((df['file_path'].values, df['variety_label'].values))\n",
    "    dataset = dataset.map(parse_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    if is_training:\n",
    "        dataset = dataset.shuffle(buffer_size=1000)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "# Data augmentation\n",
    "data_augmentation = Sequential([\n",
    "    layers.Resizing(image_size, image_size),\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(factor=0.02),\n",
    "    layers.RandomZoom(height_factor=0.2, width_factor=0.2),\n",
    "], name=\"data_augmentation\")\n",
    "\n",
    "# Normalization layer\n",
    "normalization = layers.Normalization()\n",
    "sample_dataset = create_dataset_from_df(file_df).take(5)\n",
    "normalization.adapt(sample_dataset.map(lambda x, y: x))\n",
    "\n",
    "# Multilayer perceptron (MLP)\n",
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "\n",
    "# Patch creation layer\n",
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super(Patches, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "        return patches\n",
    "\n",
    "# Patch encoding layer\n",
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super(PatchEncoder, self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded\n",
    "\n",
    "# ViT model\n",
    "def create_vit_variety_classifier():\n",
    "    inputs = layers.Input(shape=(img_height, img_width, 3))\n",
    "    normalized = normalization(inputs)\n",
    "    augmented = data_augmentation(normalized)\n",
    "    patches = Patches(patch_size)(augmented)\n",
    "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "\n",
    "    for _ in range(transformer_layers):\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "        )(x1, x1)\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = layers.Flatten()(representation)\n",
    "    representation = layers.Dropout(0.5)(representation)\n",
    "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
    "    logits = layers.Dense(num_varieties, activation='softmax', name='variety_output')(features)\n",
    "    return Model(inputs=inputs, outputs=logits)\n",
    "\n",
    "# Train one epoch\n",
    "def train_one_epoch(model, train_dataset, val_dataset, optimizer, loss_fn, steps_per_epoch, validation_steps, checkpoint_path):\n",
    "    model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n",
    "    checkpoint_callback = callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_path,\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        mode='max',\n",
    "        save_weights_only=False\n",
    "    )\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        epochs=1,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        validation_data=val_dataset,\n",
    "        validation_steps=validation_steps,\n",
    "        callbacks=[checkpoint_callback],\n",
    "        verbose=1\n",
    "    )\n",
    "    return float(history.history['loss'][0]), float(history.history['accuracy'][0]), \\\n",
    "           float(history.history['val_loss'][0]), float(history.history['val_accuracy'][0])\n",
    "\n",
    "# Evaluate model\n",
    "def evaluate(model, val_dataset, loss_fn, validation_steps):\n",
    "    model.compile(loss=loss_fn, metrics=['accuracy'])\n",
    "    results = model.evaluate(val_dataset, steps=validation_steps, verbose=1)\n",
    "    return float(results[0]), float(results[1])\n",
    "\n",
    "# Run Stratified K-Fold training\n",
    "def run_kfold_training(file_df, n_splits=4, num_epochs=50, batch_size=32):\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    loss_fn = losses.SparseCategoricalCrossentropy()\n",
    "    epoch_results = []\n",
    "    fold_best_val_acc = {f'fold_{i+1}': 0.0 for i in range(n_splits)}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "        fold_train_losses, fold_train_accs, fold_val_losses, fold_val_accs = [], [], [], []\n",
    "\n",
    "        for fold, (train_idx, val_idx) in enumerate(skf.split(file_df, file_df['variety_label'])):\n",
    "            print(f\"  Fold {fold + 1}/{n_splits}\")\n",
    "\n",
    "            train_df = file_df.iloc[train_idx].reset_index(drop=True)\n",
    "            val_df = file_df.iloc[val_idx].reset_index(drop=True)\n",
    "\n",
    "            train_dataset = create_dataset_from_df(train_df, batch_size=batch_size, is_training=True)\n",
    "            val_dataset = create_dataset_from_df(val_df, batch_size=batch_size, is_training=False)\n",
    "\n",
    "            steps_per_epoch = len(train_df) // batch_size\n",
    "            validation_steps = len(val_df) // batch_size\n",
    "\n",
    "            model = create_vit_variety_classifier()\n",
    "            optimizer = optimizers.AdamW(learning_rate=learning_rate, weight_decay=weight_decay)\n",
    "            checkpoint_path = CHECKPOINT_MODEL_PATH.format(fold=fold + 1)\n",
    "\n",
    "            train_loss, train_acc, val_loss, val_acc = train_one_epoch(\n",
    "                model, train_dataset, val_dataset, optimizer, loss_fn, steps_per_epoch, validation_steps, checkpoint_path\n",
    "            )\n",
    "            fold_train_losses.append(train_loss)\n",
    "            fold_train_accs.append(train_acc)\n",
    "            fold_val_losses.append(val_loss)\n",
    "            fold_val_accs.append(val_acc)\n",
    "\n",
    "            print(f\"    Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "            print(f\"    Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "            # Track best validation accuracy for this fold\n",
    "            if val_acc > fold_best_val_acc[f'fold_{fold + 1}']:\n",
    "                fold_best_val_acc[f'fold_{fold + 1}'] = val_acc\n",
    "\n",
    "        mean_train_loss = np.mean(fold_train_losses)\n",
    "        mean_train_acc = np.mean(fold_train_accs)\n",
    "        mean_val_loss = np.mean(fold_val_losses)\n",
    "        mean_val_acc = np.mean(fold_val_accs)\n",
    "        print(f\"Epoch {epoch + 1} Summary:\")\n",
    "        print(f\"  Mean Train Loss: {mean_train_loss:.4f}, Mean Train Acc: {mean_train_acc:.4f}\")\n",
    "        print(f\"  Mean Val Loss: {mean_val_loss:.4f}, Mean Val Acc: {mean_val_acc:.4f}\")\n",
    "\n",
    "        epoch_results.append({\n",
    "            'epoch': epoch + 1,\n",
    "            'mean_train_loss': mean_train_loss,\n",
    "            'mean_train_acc': mean_train_acc,\n",
    "            'mean_val_loss': mean_val_loss,\n",
    "            'mean_val_acc': mean_val_acc\n",
    "        })\n",
    "\n",
    "    print(\"\\nTraining Completed!\")\n",
    "    print(\"Final Results:\")\n",
    "    for result in epoch_results:\n",
    "        print(f\"Epoch {result['epoch']}: \"\n",
    "              f\"Mean Train Loss: {result['mean_train_loss']:.4f}, \"\n",
    "              f\"Mean Train Acc: {result['mean_train_acc']:.4f}, \"\n",
    "              f\"Mean Val Loss: {result['mean_val_loss']:.4f}, \"\n",
    "              f\"Mean Val Acc: {result['mean_val_acc']:.4f}\")\n",
    "\n",
    "    return epoch_results, fold_best_val_acc\n",
    "\n",
    "# Plot Learning Curves\n",
    "def plot_training_curves(epoch_results):\n",
    "    epochs = [r['epoch'] for r in epoch_results]\n",
    "    train_loss = [r['mean_train_loss'] for r in epoch_results]\n",
    "    val_loss = [r['mean_val_loss'] for r in epoch_results]\n",
    "    train_acc = [r['mean_train_acc'] for r in epoch_results]\n",
    "    val_acc = [r['mean_val_acc'] for r in epoch_results]\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_loss, label='Mean Train Loss')\n",
    "    plt.plot(epochs, val_loss, label='Mean Val Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_acc, label='Mean Train Acc')\n",
    "    plt.plot(epochs, val_acc, label='Mean Val Acc')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create test dataset\n",
    "def create_test_dataset(test_path, batch_size=32):\n",
    "    test_files = [os.path.join(test_path, img) for img in os.listdir(test_path) if img.endswith('.jpg')]\n",
    "    test_ids = [img for img in os.listdir(test_path) if img.endswith('.jpg')]\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((test_files, [0] * len(test_files)))\n",
    "    dataset = dataset.map(parse_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset, test_ids\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f717f024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/50\n",
      "  Fold 1/4\n",
      "WARNING:tensorflow:From d:\\conda\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:219: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m505s\u001b[0m 2s/step - accuracy: 0.6018 - loss: 2.2956 - val_accuracy: 0.6447 - val_loss: 1.1874\n",
      "    Train Loss: 1.5593, Train Acc: 0.6420\n",
      "    Val Loss: 1.1874, Val Acc: 0.6447\n",
      "  Fold 2/4\n",
      "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m307s\u001b[0m 1s/step - accuracy: 0.6090 - loss: 2.3096 - val_accuracy: 0.6655 - val_loss: 1.2268\n",
      "    Train Loss: 1.5715, Train Acc: 0.6493\n",
      "    Val Loss: 1.2268, Val Acc: 0.6655\n",
      "  Fold 3/4\n",
      "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m350s\u001b[0m 1s/step - accuracy: 0.6030 - loss: 2.2965 - val_accuracy: 0.6246 - val_loss: 1.3524\n",
      "    Train Loss: 1.6009, Train Acc: 0.6448\n",
      "    Val Loss: 1.3524, Val Acc: 0.6246\n",
      "  Fold 4/4\n",
      "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m308s\u001b[0m 1s/step - accuracy: 0.6081 - loss: 2.3488 - val_accuracy: 0.6744 - val_loss: 1.4150\n",
      "    Train Loss: 1.5990, Train Acc: 0.6404\n",
      "    Val Loss: 1.4150, Val Acc: 0.6744\n",
      "Epoch 1 Summary:\n",
      "  Mean Train Loss: 1.5827, Mean Train Acc: 0.6441\n",
      "  Mean Val Loss: 1.2954, Mean Val Acc: 0.6523\n",
      "\n",
      "Epoch 2/50\n",
      "  Fold 1/4\n",
      "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m289s\u001b[0m 1s/step - accuracy: 0.5942 - loss: 2.3829 - val_accuracy: 0.6782 - val_loss: 1.1851\n",
      "    Train Loss: 1.6107, Train Acc: 0.6417\n",
      "    Val Loss: 1.1851, Val Acc: 0.6782\n",
      "  Fold 2/4\n",
      "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m369s\u001b[0m 2s/step - accuracy: 0.6068 - loss: 2.2373 - val_accuracy: 0.6555 - val_loss: 1.4057\n",
      "    Train Loss: 1.5288, Train Acc: 0.6512\n",
      "    Val Loss: 1.4057, Val Acc: 0.6555\n",
      "  Fold 3/4\n",
      "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m580s\u001b[0m 2s/step - accuracy: 0.6067 - loss: 2.1574 - val_accuracy: 0.6470 - val_loss: 1.2447\n",
      "    Train Loss: 1.5503, Train Acc: 0.6462\n",
      "    Val Loss: 1.2447, Val Acc: 0.6470\n",
      "  Fold 4/4\n",
      "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m460s\u001b[0m 2s/step - accuracy: 0.5943 - loss: 2.2431 - val_accuracy: 0.6640 - val_loss: 1.3149\n",
      "    Train Loss: 1.5849, Train Acc: 0.6349\n",
      "    Val Loss: 1.3149, Val Acc: 0.6640\n",
      "Epoch 2 Summary:\n",
      "  Mean Train Loss: 1.5687, Mean Train Acc: 0.6435\n",
      "  Mean Val Loss: 1.2876, Mean Val Acc: 0.6612\n",
      "\n",
      "Epoch 3/50\n",
      "  Fold 1/4\n",
      "\u001b[1m 83/243\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m4:34\u001b[0m 2s/step - accuracy: 0.5655 - loss: 3.0528"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Run training\n",
    "    epoch_results, fold_best_val_acc = run_kfold_training(file_df, n_splits=n_splits, num_epochs=num_epochs, batch_size=batch_size)\n",
    "\n",
    "    import json\n",
    "    with open('training_results.json', 'w') as f:\n",
    "        json.dump({\n",
    "            'epoch_results': epoch_results,\n",
    "            'fold_best_val_acc': fold_best_val_acc\n",
    "        }, f)\n",
    "    print(\"Training results saved to 'training_results.json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b0266b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Check if required variables are defined\n",
    "if 'epoch_results' not in globals() or 'fold_best_val_acc' not in globals():\n",
    "    print(\"Loading training results from file...\")\n",
    "    try:\n",
    "        with open('training_results.json', 'r') as f:\n",
    "            data = json.load(f)\n",
    "            epoch_results = data['epoch_results']\n",
    "            fold_best_val_acc = data['fold_best_val_acc']\n",
    "    except FileNotFoundError:\n",
    "        raise ValueError(\"Training results not found. Please run the training cell first.\")\n",
    "\n",
    "# Plot learning curves\n",
    "print(\"Plotting learning curves...\")\n",
    "plot_training_curves(epoch_results)\n",
    "\n",
    "# Select the best model\n",
    "best_fold = max(fold_best_val_acc, key=fold_best_val_acc.get)\n",
    "best_val_acc = fold_best_val_acc[best_fold]\n",
    "best_fold_num = int(best_fold.split('_')[1])\n",
    "best_model_path = CHECKPOINT_MODEL_PATH.format(fold=best_fold_num)\n",
    "print(f\"Best model from {best_fold} with Val Acc: {best_val_acc:.4f}\")\n",
    "\n",
    "# Check if model file exists\n",
    "if not os.path.exists(best_model_path):\n",
    "    raise FileNotFoundError(f\"Best model file not found at {best_model_path}. Ensure training completed successfully.\")\n",
    "\n",
    "# Load the best model\n",
    "best_model = tf.keras.models.load_model(best_model_path)\n",
    "\n",
    "# Create test dataset\n",
    "print(\"Creating test dataset...\")\n",
    "test_pred_dataset, test_image_ids = create_test_dataset(TEST_IMG_PATH)\n",
    "\n",
    "# Generate predictions\n",
    "print(\"Generating predictions...\")\n",
    "predictions = best_model.predict(test_pred_dataset)\n",
    "predicted_variety_indices = np.argmax(predictions, axis=1)\n",
    "predicted_varieties = variety_encoder.inverse_transform(predicted_variety_indices)\n",
    "\n",
    "# Create submission dataframe\n",
    "submission_df = pd.DataFrame({\n",
    "    'image_id': test_image_ids,\n",
    "    'variety': predicted_varieties\n",
    "})\n",
    "submission_df.to_csv('variety_predictions.csv', index=False)\n",
    "print(\"Predictions saved to 'variety_predictions.csv'\")\n",
    "\n",
    "# Create detailed submission file\n",
    "confidence_df = pd.DataFrame({\n",
    "    'image_id': test_image_ids,\n",
    "    'variety': predicted_varieties,\n",
    "    'confidence': np.max(predictions, axis=1)\n",
    "})\n",
    "for i in range(3):\n",
    "    top_n_indices = np.argsort(predictions, axis=1)[:, -(i+1)]\n",
    "    confidence_df[f'variety_top_{i+1}'] = variety_encoder.inverse_transform(top_n_indices)\n",
    "    confidence_df[f'confidence_top_{i+1}'] = np.sort(predictions, axis=1)[:, -(i+1)]\n",
    "confidence_df.to_csv('variety_predictions_detailed.csv', index=False)\n",
    "print(\"Detailed predictions saved to 'variety_predictions_detailed.csv'\")\n",
    "\n",
    "# Evaluate on validation set of the best fold\n",
    "print(\"Evaluating best model on validation set...\")\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(file_df, file_df['variety_label'])):\n",
    "    if fold + 1 == best_fold_num:\n",
    "        val_df = file_df.iloc[val_idx].reset_index(drop=True)\n",
    "        val_dataset = create_dataset_from_df(val_df, batch_size=batch_size, is_training=False)\n",
    "        break\n",
    "\n",
    "val_predictions = []\n",
    "val_true_labels = []\n",
    "for batch in val_dataset:\n",
    "    images, labels = batch\n",
    "    preds = best_model.predict(images, verbose=0)\n",
    "    val_predictions.extend(np.argmax(preds, axis=1))\n",
    "    val_true_labels.extend(labels.numpy())\n",
    "\n",
    "# Create classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(val_true_labels, val_predictions, target_names=variety_encoder.classes_))\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(val_true_labels, val_predictions)\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=variety_encoder.classes_,\n",
    "            yticklabels=variety_encoder.classes_)\n",
    "plt.title('Confusion Matrix for Variety Classification')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTraining and evaluation completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
