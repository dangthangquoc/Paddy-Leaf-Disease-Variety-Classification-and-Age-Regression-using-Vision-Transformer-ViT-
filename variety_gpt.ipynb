{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "931a80fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mF\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Rice Variety Classification Notebook\n",
    "# Combined code for variety classification using CBAM-ResNet18\n",
    "\n",
    "# %%\n",
    "# 1. Imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, models\n",
    "from torchvision.io import decode_image\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from timm.data import Mixup\n",
    "from torchmetrics.classification import MulticlassF1Score\n",
    "\n",
    "# %%\n",
    "# 2. Variety Information (optional)\n",
    "VARIETY_INFO = {\n",
    "    \"Basmati\": {\n",
    "        \"origin\": \"India/Pakistan\",\n",
    "        \"characteristics\": \"Long grain, aromatic\",\n",
    "        \"growing_period\": \"120-150 days\",\n",
    "        \"optimal_conditions\": \"Warm climate, well-drained soil\"\n",
    "    },\n",
    "    \"Jasmine\": {\n",
    "        \"origin\": \"Thailand\",\n",
    "        \"characteristics\": \"Long grain, fragrant\",\n",
    "        \"growing_period\": \"110-120 days\",\n",
    "        \"optimal_conditions\": \"Tropical climate, abundant water\"\n",
    "    },\n",
    "    \"Arborio\": {\n",
    "        \"origin\": \"Italy\",\n",
    "        \"characteristics\": \"Medium grain, high starch content\",\n",
    "        \"growing_period\": \"130-150 days\",\n",
    "        \"optimal_conditions\": \"Temperate climate, consistent water\"\n",
    "    },\n",
    "    \"Sushi\": {\n",
    "        \"origin\": \"Japan\",\n",
    "        \"characteristics\": \"Short grain, sticky when cooked\",\n",
    "        \"growing_period\": \"120-140 days\",\n",
    "        \"optimal_conditions\": \"Temperate climate, consistent water level\"\n",
    "    },\n",
    "    \"Long Grain\": {\n",
    "        \"origin\": \"Various regions\",\n",
    "        \"characteristics\": \"Long and slender grain, fluffy when cooked\",\n",
    "        \"growing_period\": \"110-130 days\",\n",
    "        \"optimal_conditions\": \"Warm climate, good irrigation\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# %%\n",
    "# 3. Image Transform\n",
    "\n",
    "def get_transform():\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "# %%\n",
    "# 4. Attention Modules & Model Definition\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_channels, reduction_ratio=16):\n",
    "        super().__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_channels, in_channels // reduction_ratio),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_channels // reduction_ratio, in_channels)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        max_feats = self.max_pool(x)\n",
    "        avg_feats = self.avg_pool(x)\n",
    "        max_feats = torch.flatten(max_feats, 1)\n",
    "        avg_feats = torch.flatten(avg_feats, 1)\n",
    "        max_out = self.mlp(max_feats)\n",
    "        avg_out = self.mlp(avg_feats)\n",
    "        scale = self.sigmoid(max_out + avg_out).unsqueeze(2).unsqueeze(3)\n",
    "        return x * scale\n",
    "\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size=kernel_size,\n",
    "                              padding=kernel_size // 2)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        max_result, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        avg_result = torch.mean(x, dim=1, keepdim=True)\n",
    "        result = torch.cat([max_result, avg_result], dim=1)\n",
    "        attn = self.sigmoid(self.conv(result))\n",
    "        return x * attn\n",
    "\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, in_channels, reduction_ratio=16, kernel_size=7):\n",
    "        super().__init__()\n",
    "        self.ca = ChannelAttention(in_channels, reduction_ratio)\n",
    "        self.sa = SpatialAttention(kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ca(x)\n",
    "        x = self.sa(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CBAMResNet18(nn.Module):\n",
    "    def __init__(self, num_classes, in_channels=3):\n",
    "        super().__init__()\n",
    "        base = models.resnet18(weights=None)\n",
    "        if in_channels != 3:\n",
    "            base.conv1 = nn.Conv2d(in_channels, 64, kernel_size=7,\n",
    "                                    stride=2, padding=3, bias=False)\n",
    "        self.stem = nn.Sequential(base.conv1, base.bn1,\n",
    "                                  base.relu, base.maxpool)\n",
    "        self.layer1, self.layer2, self.layer3, self.layer4 = (\n",
    "            base.layer1, base.layer2, base.layer3, base.layer4)\n",
    "        self.cbam1 = CBAM(64)\n",
    "        self.cbam2 = CBAM(128)\n",
    "        self.cbam3 = CBAM(256)\n",
    "        self.cbam4 = CBAM(512)\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(32, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.cbam1(self.layer1(x))\n",
    "        x = self.cbam2(self.layer2(x))\n",
    "        x = self.cbam3(self.layer3(x))\n",
    "        x = self.cbam4(self.layer4(x))\n",
    "        x = self.pool(x)\n",
    "        return self.classifier(x)\n",
    "\n",
    "# %%\n",
    "# 5. Dataset & DataLoader for Variety Classification\n",
    "class RiceDataset(Dataset):\n",
    "    def __init__(self, image_dir, labels_path, split=\"train\",\n",
    "                 transform=None, val_size=0.2, random_seed=42,\n",
    "                 oversample=False):\n",
    "        df = pd.read_csv(labels_path)\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        train_df, val_df = train_test_split(\n",
    "            df, test_size=val_size, stratify=df[\"variety\"],\n",
    "            random_state=random_seed)\n",
    "        self.metadata = train_df if split == \"train\" else val_df\n",
    "        if oversample and split == \"train\":\n",
    "            from sklearn.utils import resample\n",
    "            class_dfs = []\n",
    "            max_size = self.metadata[\"variety\"].value_counts().max()\n",
    "            for cls, grp in self.metadata.groupby(\"variety\"):\n",
    "                up = resample(grp, replace=True,\n",
    "                              n_samples=max_size,\n",
    "                              random_state=random_seed)\n",
    "                class_dfs.append(up)\n",
    "            self.metadata = pd.concat(class_dfs).sample(frac=1,\n",
    "                               random_state=random_seed)\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.classes = sorted(self.metadata[\"variety\"].unique())\n",
    "        self.class_to_idx = {c:i for i,c in enumerate(self.classes)}\n",
    "        self.image_paths = []\n",
    "        self.targets = []\n",
    "        for _, row in self.metadata.iterrows():\n",
    "            folder = row[\"label\"]\n",
    "            img_id = row[\"image_id\"]\n",
    "            path = os.path.join(image_dir, folder, img_id)\n",
    "            self.image_paths.append(path)\n",
    "            self.targets.append(self.class_to_idx[row[\"variety\"]])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = decode_image(self.image_paths[idx])\n",
    "        img = to_pil_image(img)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        label = torch.tensor(self.targets[idx], dtype=torch.long)\n",
    "        return img, label\n",
    "\n",
    "\n",
    "def get_dataloaders(image_dir, labels_path, batch_size=32,\n",
    "                    val_size=0.2, oversample=False):\n",
    "    train_ds = RiceDataset(image_dir, labels_path, split=\"train\",\n",
    "                            transform=get_transform(),\n",
    "                            val_size=val_size, oversample=oversample)\n",
    "    val_ds   = RiceDataset(image_dir, labels_path, split=\"val\",\n",
    "                            transform=get_transform(),\n",
    "                            val_size=val_size)\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size,\n",
    "                              shuffle=True, num_workers=4,\n",
    "                              pin_memory=True,\n",
    "                              persistent_workers=True)\n",
    "    val_loader   = DataLoader(val_ds, batch_size=batch_size,\n",
    "                              shuffle=False, num_workers=4,\n",
    "                              pin_memory=True,\n",
    "                              persistent_workers=True)\n",
    "    return train_loader, val_loader\n",
    "\n",
    "# %%\n",
    "# 6. Training Utilities & Trainer\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0.001):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.best_state = None\n",
    "\n",
    "    def early_stop(self, loss, model):\n",
    "        if loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = loss\n",
    "            self.counter = 0\n",
    "            self.best_state = model.state_dict()\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "# Optional Mixup (set mixup=False if not needed)\n",
    "mixup_fn = Mixup(\n",
    "    mixup_alpha=0.4,\n",
    "    cutmix_alpha=1.0,\n",
    "    prob=0.5,\n",
    "    switch_prob=0.5,\n",
    "    mode='batch',\n",
    "    label_smoothing=0.1,\n",
    "    num_classes=len(VARIETY_INFO)\n",
    ")\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, loss_fn, optimizer, metric,\n",
    "                 device, model_name, scheduler=None, save=True,\n",
    "                 mixup=False):\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.metric = metric\n",
    "        self.device = device\n",
    "        self.scheduler = scheduler\n",
    "        self.save = save\n",
    "        self.model_name = model_name\n",
    "        self.mixup = mixup\n",
    "        self.history = {\"train_loss\":[], \"val_loss\":[],\n",
    "                        \"train_f1\":[], \"val_f1\":[], \"lr\":[]}\n",
    "\n",
    "    def train_epoch(self, loader):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        self.metric.reset()\n",
    "        for imgs, labels in loader:\n",
    "            imgs, labels = imgs.to(self.device), labels.to(self.device)\n",
    "            if self.mixup:\n",
    "                imgs, labels = mixup_fn(imgs, labels)\n",
    "            preds = self.model(imgs)\n",
    "            loss = self.loss_fn(preds, labels)\n",
    "            total_loss += loss.item()\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.metric(preds.argmax(1), labels)\n",
    "        avg_loss = total_loss / len(loader)\n",
    "        f1 = self.metric.compute().item()\n",
    "        lr = self.optimizer.param_groups[0]['lr']\n",
    "        self.history['train_loss'].append(avg_loss)\n",
    "        self.history['train_f1'].append(f1)\n",
    "        self.history['lr'].append(lr)\n",
    "        return avg_loss, f1\n",
    "\n",
    "    def val_epoch(self, loader):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        self.metric.reset()\n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in loader:\n",
    "                imgs, labels = imgs.to(self.device), labels.to(self.device)\n",
    "                preds = self.model(imgs)\n",
    "                loss = self.loss_fn(preds, labels)\n",
    "                total_loss += loss.item()\n",
    "                self.metric(preds.argmax(1), labels)\n",
    "        avg_loss = total_loss / len(loader)\n",
    "        f1 = self.metric.compute().item()\n",
    "        self.history['val_loss'].append(avg_loss)\n",
    "        self.history['val_f1'].append(f1)\n",
    "        return avg_loss, f1\n",
    "\n",
    "    def fit(self, train_loader, val_loader, epochs=10):\n",
    "        stopper = EarlyStopping()\n",
    "        for epoch in range(epochs):\n",
    "            tr_loss, tr_f1 = self.train_epoch(train_loader)\n",
    "            val_loss, val_f1 = self.val_epoch(val_loader)\n",
    "            if stopper.early_stop(val_loss, self.model):\n",
    "                print(\"Early stopping at epoch\", epoch+1)\n",
    "                break\n",
    "        if self.save and stopper.best_state:\n",
    "            torch.save(stopper.best_state, f\"{self.model_name}.pt\")\n",
    "            pd.DataFrame(self.history).to_csv(f\"{self.model_name}_history.csv\", index=False)\n",
    "        return self.history\n",
    "\n",
    "# %%\n",
    "# 7. Example Training & Inference\n",
    "# Paths (change to your data)\n",
    "HOME_PATH = os.getcwd() + \"/\"\n",
    "image_dir = HOME_PATH + 'train_images'\n",
    "labels_csv = pd.read_csv(HOME_PATH + \"meta_train.csv\")\n",
    "\n",
    "# Get data\n",
    "train_loader, val_loader = get_dataloaders(image_dir, labels_csv,\n",
    "                                           batch_size=32, oversample=True)\n",
    "\n",
    "# Setup model, loss, optimizer, metric\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = CBAMResNet18(num_classes=len(VARIETY_INFO)).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "metric = MulticlassF1Score(num_classes=len(VARIETY_INFO)).to(device)\n",
    "\n",
    "# Train\n",
    "trainer = Trainer(model, criterion, optimizer, metric, device,\n",
    "                  model_name=\"variety_model\", mixup=False)\n",
    "history = trainer.fit(train_loader, val_loader, epochs=10)\n",
    "\n",
    "# %%\n",
    "# Inference Function\n",
    "\n",
    "def predict_variety(image_path, model, device):\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    tensor = get_transform()(img).unsqueeze(0).to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(tensor)\n",
    "        probs = F.softmax(out, dim=1)[0]\n",
    "        idx = torch.argmax(probs).item()\n",
    "        names = list(VARIETY_INFO.keys())\n",
    "        return names[idx], probs[idx].item()\n",
    "\n",
    "# Usage\n",
    "# pred_name, pred_conf = predict_variety(\"test.jpg\", model, device)\n",
    "# print(f\"Predicted: {pred_name} ({pred_conf*100:.2f}%)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paddy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
